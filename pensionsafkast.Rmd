---
title: "Pension returns analysis"
header-includes:
   - \usepackage[default]{sourcesanspro}
   - \usepackage[T1]{fontenc}
   - \usepackage[fontsize=8pt]{scrextend}
mainfont: SourceSansPro
output: 
  html_document:
    toc: true
    toc_depth: 3
    keep_md: yes
  # pdf_document:
  #   toc: true
  #   toc_depth: 3
#fontsize: 10pt # for pdf. Limited to 10pt, 11pt and 12pt. Else use scrextend.
params:
  run_sim: TRUE ## TRUE: Run simulations and write output. FALSE: Read saved
                ## simulations output from disk instead of running the simulations.
date: "`r format(Sys.time(), '%H:%M %d %B %Y')`"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, warning=FALSE, message=FALSE)
```

```{r}
source("~/R\ work/pension-returns/pension-returns_functions.R")
```


```{r include=FALSE}
## ****************************** WARNING ******************************
## 
##             This document takes a long time to render!
##
## *********************************************************************
##
## To read previous simulation outputs from disk rather than run simulations,
## set YAML parameter run_sim: TRUE
##
## NOTE:
## When using the "Run All" command, YAML parameters will be ignored. An error
## message "Error in eval(ele) : object 'params' not found" will be printed,
## but the chunks will be evaluated.
```



```{r eval=FALSE, include=FALSE}
## Render this document to html and pdf
rmarkdown::render("pensionsafkast.Rmd", output_format ="all")
```


```{r}
mc_num_paths <- 10000
mc_num_periods <- 20
mc_dao <- TRUE
```



Fit log returns to F-S skew standardized Student-t distribution.  
`m`  is the location parameter.  
`s` is the scale parameter.  
`nu` is the estimated shape parameter (degrees of freedom).  
`xi` is the estimated skewness parameter.  

# Log returns data 2011-2023.  
For 2011, medium risk data is used in the high risk data set, as no high risk fund data is available prior to 2012.  
`vmrl` is a long version of Velliv medium risk data, from 2007 to 2023. For 2007 to 2011 (both included) no high risk data is available.
```{r}
# Velliv medium risk returns
vmr <- log(c(0.986, 1.098, 1.157, 1.136, 1.058, 1.044, 1.100, 0.963, 1.168, 1.097, 1.141, 0.868, 1.094))

# Velliv medium risk return, long period: 2007-2023
vmrl <- log(c(1.058, 0.801, 1.193, 1.129, 1.035, 0.996, 1.133, 1.092, 1.085, 1.013, 1.095, 1.011, 1.129, 1.101, 1.128, 0.988, 1.048))

# PFA medium risk returns
pmr <- log(c(1.084, 0.904, 1.107, 1.042, 1.119, 0.976, 1.089, 1.073, 1.084, 1.107, 1.111, 1.141, 1.004))

# Mix medium risk return
mmr <- log(c(1.035, 0.996, 1.133, 1.092, 1.085, 1.013, 1.095, 1.011, 1.129, 1.101, 1.128, 0.988, 1.048))

# Velliv high risk returns
vhr <- log(c(0.986, 1.093, 1.214, 1.160, 1.075, 1.039, 1.118, 0.952, 1.205, 1.099, 1.194, 0.849, 1.122))

# PFA high risk return
phr <- log(c(1.159, 0.878, 1.200, 1.068, 1.190, 0.943, 1.135, 1.082, 1.123, 1.128, 1.208, 1.182, 0.934))
  
# Mix high risk return
mhr <- log(c(1.073, 0.977, 1.207, 1.116, 1.128, 0.992, 1.126, 1.013, 1.164, 1.113, 1.201, 1.012, 1.014))

vm_ph_r <- (c(100, 100 * cumprod(exp(vmr))) + c(100, 100 * cumprod(exp(phr))))/2
vm_ph_r <- log(tail(vm_ph_r, -1)/head(vm_ph_r, -1))

vh_pm_r <- (c(100, 100 * cumprod(exp(vhr))) + c(100, 100 * cumprod(exp(pmr))))/2
vh_pm_r <- log(tail(vh_pm_r, -1)/head(vh_pm_r, -1))

data_df <- data.frame(
  vmr = vmr,
  vhr = vhr,
  pmr = pmr,
  phr = phr,
  mmr = mmr,
  mhr = mhr,
  vm_ph_r = vm_ph_r,
  vh_pm_r = vh_pm_r
)

data_df_l <- data.frame(
  vmrl = vmrl
)
```






```{r}
all_data <- exp(c(vmr, vhr, pmr, phr, mmr, mhr))
plot(x = c(2011:2023), y = exp(vmr), type = "l",  lty = 1, lwd = 2, xlab = "year", ylab = "Gross returns", col = "#7AC5CD", ylim = c(min(all_data), max(all_data)), main = "Gross returns 2011-2023")
lines(x = c(2011:2023), y = exp(vhr), col = "#76EEC6",  lty = 2, lwd = 1.5)
lines(x = c(2011:2023), y = exp(pmr), col = "#CD6600",  lty = 1, lwd = 2)
lines(x = c(2011:2023), y = exp(phr), col = "#CDAD00",  lty = 2, lwd = 1.5)
lines(x = c(2011:2023), y = exp(mmr), col = "#A020F0",  lty = 1, lwd = 2)
lines(x = c(2011:2023), y = exp(mhr), col = "#FF00FF",  lty = 2, lwd = 1.5)
legend("bottom", legend = c("vmr", "vhr", "pmr", "phr", "mmr", "mhr"), col = c("#7AC5CD", "#76EEC6", "#CD6600", "#CDAD00", "#A020F0", "#FF00FF"), lty = c(1, 2, 1, 2, 1, 2), lwd = 2, ncol = 3 )
```


## Summary of gross returns

```{r}
df_summary <- summary(exp(data_df))
df_summary
```


```{r}
summary(exp(data_df_l))
```

```{r}
summary_df <- df_summary_to_df(df_summary)
summary_df
```

## Ranking
```{r}
knitr::kable(rank_summary(summary_df, sorting = rep("hi", ncol(summary_df))), digits = 3)
```

## Covariance and correlations


Covariances
```{r}
round(cov(data.frame(vmr, vhr, pmr, phr)), 4)
```

Correlations
```{r}
round(cor(data.frame(vmr, vhr, pmr, phr)), 4)
```
`vhr` and `phr` are clearly the least correlated.




# Velliv medium risk, 2011 - 2023
```{r}
fit_vmr <- fit_skewed_t(vmr)
```

## QQ Plot
```{r}
(fit_vmr$qqplot)()
```

The qq plot looks great. Log returns for Velliv medium risk seems to be consistent with a skewed t-distribution.  


## Data vs fit
Let's plot the fit and the observed returns together.  

```{r}
(fit_vmr$fit_plot)()
```

## Estimated distribution
Now lets look at the CDF of the estimated distribution for each 0.1% increment between 0.5% and 99.5% for the estimated distribution:

```{r}
(fit_vmr$dist_plot)()
```


```{r}
(fit_vmr$quantile_plot)()
```

We see that for a few observations out of a 1000, the losses are disastrous, while the upside is very dampened.


```{r}
(fit_vmr$dens_plot)()
```


## Monte Carlo

```{r eval=params$run_sim}
mc_vmr <- mc_simulation(fit_vmr, num_paths = mc_num_paths, num_periods = mc_num_periods, dao = mc_dao)
saveRDS(mc_vmr, file="mc_vmr.RData")
```

```{r}
mc_vmr <- readRDS(file="mc_vmr.RData")
```


```{r results='hide',fig.keep='all'}
(mc_vmr$mc_plot)()
```


```{r results='hide',fig.keep='all'}
(mc_vmr$mc_plot_last_period)()
```




## Convergence

### Max vs sum
Max vs sum plots for the first four moments:

```{r eval=params$run_sim}
vmr_max_sum_plot <- plot_max_sum(fit_vmr, 1e4)
saveRDS(vmr_max_sum_plot, "vmr_max_sum_plot.RData")
```

```{r}
vmr_max_sum_plot <- readRDS(file="vmr_max_sum_plot.RData")
vmr_max_sum_plot
```




### MC

```{r}
mc_vmr$mc_conv_plot()
```

### IS


```{r eval=params$run_sim}
is_g_fit_vmr <- is_proposal(
  x_i_fit = fit_vmr,
  x_n_vect = x_n_vect_from_mc_df(mc_vmr$mc_df),
  num_paths = mc_num_paths, 
  num_periods = mc_num_periods, 
  obj_func_plot = TRUE,
  init_par = c(2, 0.5),
  method = "L-BFGS-B",
  lower = c(1, 0.1),
  upper = c(3, 0.9)
)
saveRDS(is_g_fit_vmr, "is_g_fit_vmr.RData")
```

```{r}
is_g_fit_vmr <- readRDS(file="is_g_fit_vmr.RData")
```


Parameters
```{r}
is_g_fit_vmr$par
```

Objective function plots

```{r}
is_g_fit_vmr$mean_vect_plot()
```

```{r}
is_g_fit_vmr$sd_vect_plot()
```





```{r eval=params$run_sim}
is_output_vmr <- importance_sampling(
  x_i_fit = fit_vmr, 
  x_n_vect = x_n_vect_from_mc_df(mc_vmr$mc_df),
  num_paths = mc_num_paths, 
  num_periods = mc_num_periods, 
  g_n_params = is_g_fit_vmr$par, 
  mode = 1)
saveRDS(is_output_vmr, "is_output_vmr.RData")
```


```{r}
is_output_vmr <- readRDS(file="is_output_vmr.RData")
```


```{r}
is_output_vmr$is_plot()
```


# Velliv medium risk, 2007 - 2023

## Fit to skew t distribution
```{r}
fit_vmrl <- fit_skewed_t(vmrl)
```


## QQ Plot

```{r}
(fit_vmrl$qqplot)()
```

The qq plot looks good. Log returns for Velliv high risk seems to be consistent with a skewed t-distribution.  


## Data vs fit
Let's plot the fit and the observed returns together.  

```{r}
(fit_vmrl$fit_plot)()
```


## Estimated distribution
Now lets look at the CDF of the estimated distribution for each 0.1% increment between 0.5% and 99.5% for the estimated distribution:

```{r}
(fit_vmrl$dist_plot)()
```

```{r}
(fit_vmrl$quantile_plot)()
```

We see that for a few observations out of a 1000, the losses are disastrous, while the upside is very dampened. But because the disastrous loss in 2008 was followed by a large profit the following year, we see some increased upside for the top percentiles. Beware: A 1.2 return following a 0.8 return doesn't take us back where we were before the loss. Path dependency! So if returns more or less average out, but high returns have a tendency to follow high losses, that's bad!


```{r}
(fit_vmrl$dens_plot)()
```


## Monte Carlo

```{r eval=params$run_sim}
mc_vmrl <- mc_simulation(fit_vmrl, num_paths = mc_num_paths, num_periods = mc_num_periods, dao = mc_dao)
saveRDS(mc_vmrl, file="mc_vmrl.RData")
```


```{r}
mc_vmrl <- readRDS(file="mc_vmrl.RData")
```


```{r results='hide',fig.keep='all'}
(mc_vmrl$mc_plot)()
```



```{r results='hide',fig.keep='all'}
(mc_vmrl$mc_plot_last_period)()
```


## Convergence

### Max vs sum
Max vs sum plots for the first four moments:

```{r eval=params$run_sim}
vmrl_max_sum_plot <- plot_max_sum(fit_vmrl, 1e4)
saveRDS(vmrl_max_sum_plot, "vmrl_max_sum_plot.RData")
```


```{r}
vmrl_max_sum_plot <- readRDS(file="vmrl_max_sum_plot.RData")
vmrl_max_sum_plot
```


### MC
```{r}
mc_vmrl$mc_conv_plot()
```

### IS

```{r eval=params$run_sim}
is_g_fit_vmrl <- is_proposal(
  x_i_fit = fit_vmrl,
  x_n_vect = x_n_vect_from_mc_df(mc_vmrl$mc_df),
  num_paths = mc_num_paths, 
  num_periods = mc_num_periods, 
  obj_func_plot = TRUE,
  init_par = c(2, 0.5),
  method = "L-BFGS-B",
  lower = c(1, 0.1),
  upper = c(3, 0.9)
)
saveRDS(is_g_fit_vmrl, "is_g_fit_vmrl.RData")
```

```{r}
is_g_fit_vmrl <- readRDS(file="is_g_fit_vmrl.RData")
```


Parameters
```{r}
is_g_fit_vmrl$par
```

Objective function plots

```{r}
is_g_fit_vmrl$mean_vect_plot()
```

```{r}
is_g_fit_vmrl$sd_vect_plot()
```





```{r eval=params$run_sim}
is_output_vmrl <- importance_sampling(
  x_i_fit = fit_vmrl, 
  x_n_vect = x_n_vect_from_mc_df(mc_vmrl$mc_df),
  num_paths = mc_num_paths, 
  num_periods = mc_num_periods, 
  g_n_params = is_g_fit_vmrl$par, 
  mode = 1)
saveRDS(is_output_vmrl, "is_output_vmrl.RData")
```

```{r}
is_output_vmrl <- readRDS(file="is_output_vmrl.RData")
```



```{r}
is_output_vmrl$is_plot()
```



# Velliv high risk, 2011 - 2023

## Fit to skew t distribution
```{r}
fit_vhr <- fit_skewed_t(vhr, method = "Nelder-Mead")
```


## QQ Plot
```{r}
(fit_vhr$qqplot)()
```

The qq plot looks great. Returns for Velliv medium risk seems to be consistent with a skewed t-distribution.  

## Data vs fit
Let's plot the fit and the observed returns together.  

```{r}
(fit_vhr$fit_plot)()
```

## Estimated distribution
Now lets look at the CDF of the estimated distribution for each 0.1% increment between 0.5% and 99.5% for the estimated distribution:

```{r}
(fit_vhr$dist_plot)()
```

```{r}
(fit_vhr$quantile_plot)()
```

We see that for a few observations out of a 1000, the losses are disastrous, while the upside is very dampened.


```{r}
(fit_vhr$dens_plot)()
```


## Monte Carlo

```{r eval=params$run_sim}
mc_vhr <- mc_simulation(fit_vhr, num_paths = mc_num_paths, num_periods = mc_num_periods, dao = mc_dao)
saveRDS(mc_vhr, file="mc_vhr.RData")
```


```{r}
mc_vhr <- readRDS(file="mc_vhr.RData")
```


```{r results='hide',fig.keep='all'}
(mc_vhr$mc_plot)()
```



```{r results='hide',fig.keep='all'}
(mc_vhr$mc_plot_last_period)()
```


## Convergence

### Max vs sum
Max vs sum plots for the first four moments:

```{r eval=params$run_sim}
vhr_max_sum_plot <- plot_max_sum(fit_vhr, 1e4)
saveRDS(vhr_max_sum_plot, "vhr_max_sum_plot.RData")
```


```{r}
vhr_max_sum_plot <- readRDS(file="vhr_max_sum_plot.RData")
vhr_max_sum_plot
```


### MC

```{r}
mc_vhr$mc_conv_plot()
```

### IS

```{r eval=params$run_sim}
is_g_fit_vhr <- is_proposal(
  x_i_fit = fit_vhr,
  x_n_vect = x_n_vect_from_mc_df(mc_vhr$mc_df),
  num_paths = mc_num_paths, 
  num_periods = mc_num_periods, 
  obj_func_plot = TRUE,
  init_par = c(2, 0.5),
  method = "L-BFGS-B",
  lower = c(1, 0.1),
  upper = c(3, 0.9)
)
saveRDS(is_g_fit_vhr, "is_g_fit_vhr.RData")
```

```{r}
is_g_fit_vhr <- readRDS(file="is_g_fit_vhr.RData")
```


Parameters
```{r}
is_g_fit_vhr$par
```

Objective function plots

```{r}
is_g_fit_vhr$mean_vect_plot()
```

```{r}
is_g_fit_vhr$sd_vect_plot()
```





```{r eval=params$run_sim}
is_output_vhr <- importance_sampling(
  x_i_fit = fit_vhr, 
  x_n_vect = x_n_vect_from_mc_df(mc_vhr$mc_df),
  num_paths = mc_num_paths, 
  num_periods = mc_num_periods, 
  g_n_params = is_g_fit_vhr$par, 
  mode = 1)
saveRDS(is_output_vhr, "is_output_vhr.RData")
```


```{r}
is_output_vhr <- readRDS(file="is_output_vhr.RData")
```


```{r}
is_output_vhr$is_plot()
```


# PFA medium risk, 2011 - 2023

## Fit to skew t distribution
```{r}
fit_pmr <- fit_skewed_t(pmr)
```


## QQ Plot
```{r}
(fit_pmr$qqplot)()
```

The qq plot looks great. Log returns for PFA medium risk seems to be consistent with a skewed t-distribution.  


```{r}
fit_pmr$theoretical_quantiles
```


## Data vs fit
Let's plot the fit and the observed returns together.  

```{r}
(fit_pmr$fit_plot)()
```

## Estimated distribution
Now lets look at the CDF of the estimated distribution for each 0.1% increment between 0.5% and 99.5% for the estimated distribution:

```{r}
(fit_pmr$dist_plot)()
```

```{r}
(fit_pmr$quantile_plot)()
```

We see that for a few observations out of a 1000, the losses are disastrous. While there is some uptick at the top percentiles, the curve basically flattens out.


```{r}
(fit_pmr$dens_plot)()
```


## Monte Carlo

```{r eval=params$run_sim}
mc_pmr <- mc_simulation(fit_pmr, num_paths = mc_num_paths, num_periods = mc_num_periods, dao = mc_dao)
saveRDS(mc_pmr, file="mc_pmr.RData")
```


```{r}
mc_pmr <- readRDS(file="mc_pmr.RData")
```


```{r results='hide',fig.keep='all'}
(mc_pmr$mc_plot)()
```



```{r results='hide',fig.keep='all'}
(mc_pmr$mc_plot_last_period)()
```


## Convergence

### Max vs sum

Max vs sum plots for the first four moments:

```{r eval=params$run_sim}
pmr_max_sum_plot <- plot_max_sum(fit_pmr, 1e4)
saveRDS(pmr_max_sum_plot, "pmr_max_sum_plot.RData")
```


```{r}
pmr_max_sum_plot <- readRDS(file="pmr_max_sum_plot.RData")
pmr_max_sum_plot
```


### MC

```{r}
mc_pmr$mc_conv_plot()
```

### IS

```{r eval=params$run_sim}
is_g_fit_pmr <- is_proposal(
  x_i_fit = fit_pmr,
  x_n_vect = x_n_vect_from_mc_df(mc_pmr$mc_df),
  num_paths = mc_num_paths, 
  num_periods = mc_num_periods, 
  obj_func_plot = TRUE,
  init_par = c(2, 0.5),
  method = "L-BFGS-B",
  lower = c(1, 0.1),
  upper = c(3, 0.9)
)
saveRDS(is_g_fit_pmr, "is_g_fit_pmr.RData")
```

```{r}
is_g_fit_pmr <- readRDS(file="is_g_fit_pmr.RData")
```


Parameters
```{r}
is_g_fit_pmr$par
```

Objective function plots

```{r}
is_g_fit_pmr$mean_vect_plot()
```

```{r}
is_g_fit_pmr$sd_vect_plot()
```





```{r eval=params$run_sim}
is_output_pmr <- importance_sampling(
  x_i_fit = fit_pmr, 
  x_n_vect = x_n_vect_from_mc_df(mc_pmr$mc_df),
  num_paths = mc_num_paths, 
  num_periods = mc_num_periods, 
  g_n_params = is_g_fit_pmr$par, 
  mode = 1)
saveRDS(is_output_pmr, "is_output_pmr.RData")
```

```{r}
is_output_pmr <- readRDS(file="is_output_pmr.RData")
```



```{r}
is_output_pmr$is_plot()
```



# PFA high risk, 2011 - 2023

## Fit to skew t distribution
```{r}
fit_phr <- fit_skewed_t(phr)
```


## QQ Plot
```{r}
(fit_phr$qqplot)()
```

The qq plot looks ok. Returns for PFA high risk seems to be consistent with a skewed t-distribution.  

## Data vs fit
Let's plot the fit and the observed returns together.  

```{r}
(fit_phr$fit_plot)()
```

## Estimated distribution
Now lets look at the CDF of the estimated distribution for each 0.1% increment between 0.5% and 99.5% for the estimated distribution:

```{r}
(fit_phr$dist_plot)()
```

```{r}
(fit_phr$quantile_plot)()
```

We see that for a few observations out of a 1000, the losses are disastrous, while the upside is very dampened.


```{r}
(fit_phr$dens_plot)()
```



## Monte Carlo

```{r eval=params$run_sim}
mc_phr <- mc_simulation(fit_phr, num_paths = mc_num_paths, num_periods = mc_num_periods, dao = mc_dao)
saveRDS(mc_phr, file="mc_phr.RData")
```


```{r}
mc_phr <- readRDS(file="mc_phr.RData")
```


```{r results='hide',fig.keep='all'}
(mc_phr$mc_plot)()
```



```{r results='hide',fig.keep='all'}
(mc_phr$mc_plot_last_period)()
```


## Convergence

### Max vs sum

Max vs sum plots for the first four moments:

```{r eval=params$run_sim}
phr_max_sum_plot <- plot_max_sum(fit_phr, 1e4)
saveRDS(phr_max_sum_plot, "phr_max_sum_plot.RData")
```


```{r}
phr_max_sum_plot <- readRDS(file="phr_max_sum_plot.RData")
phr_max_sum_plot
```


### MC

```{r}
mc_phr$mc_conv_plot()
```


### IS

```{r eval=params$run_sim}
is_g_fit_phr <- is_proposal(
  x_i_fit = fit_phr,
  x_n_vect = x_n_vect_from_mc_df(mc_phr$mc_df),
  num_paths = mc_num_paths, 
  num_periods = mc_num_periods, 
  obj_func_plot = TRUE,
  init_par = c(2, 0.5),
  method = "L-BFGS-B",
  lower = c(1, 0.1),
  upper = c(3, 0.9)
)
saveRDS(is_g_fit_phr, "is_g_fit_phr.RData")
```

```{r}
is_g_fit_phr <- readRDS(file="is_g_fit_phr.RData")
```


Parameters
```{r}
is_g_fit_phr$par
```

Objective function plots

```{r}
is_g_fit_phr$mean_vect_plot()
```

```{r}
is_g_fit_phr$sd_vect_plot()
```





```{r eval=params$run_sim}
is_output_phr <- importance_sampling(
  x_i_fit = fit_phr, 
  x_n_vect = x_n_vect_from_mc_df(mc_phr$mc_df),
  num_paths = mc_num_paths, 
  num_periods = mc_num_periods, 
  g_n_params = is_g_fit_phr$par, 
  mode = 1)
saveRDS(is_output_phr, "is_output_phr.RData")
```

```{r}
is_output_phr <- readRDS(file="is_output_phr.RData")
```


```{r}
is_output_phr$is_plot()
```



# Mix medium risk, 2011 - 2023

## Fit to skew t distribution
```{r}
fit_mmr <- fit_skewed_t(mmr)
```


## QQ Plot
```{r}
(fit_mmr$qqplot)()
```

The fit suggests big losses for the lowest percentiles, which are not present in the data.  
So the fit is actually a very cautious estimate.


## Data vs fit
Let's plot the fit and the observed returns together.  

```{r}
(fit_mmr$fit_plot)()
```

Interestingly, the fit predicts a much bigger "biggest loss" than the actual data. This is the main reason that R^2 is 0.90 and not higher.


## Estimated distribution
Now lets look at the CDF of the estimated distribution for each 0.1% increment between 0.5% and 99.5% for the estimated distribution:

```{r}
(fit_mmr$dist_plot)()
```

```{r}
(fit_mmr$quantile_plot)()
```

We see that for a few observations out of a 1000, the losses are disastrous, while the upside is very dampened.

```{r}
(fit_mmr$dens_plot)()
```



## Monte Carlo

### Version a: Simulation from estimated distribution of returns of mix.  
```{r eval=params$run_sim}
mc_mmr_a <- mc_simulation(fit_mmr, num_paths = mc_num_paths, num_periods = mc_num_periods, dao = mc_dao)
saveRDS(mc_mmr_a, file="mc_mmr_a.RData")
```


```{r}
mc_mmr_a <- readRDS(file="mc_mmr_a.RData")
```


```{r results='hide',fig.keep='all'}
(mc_mmr_a$mc_plot)()
```



```{r results='hide',fig.keep='all'}
(mc_mmr_a$mc_plot_last_period)()
```


```{r include=FALSE}
# NOTE: Can we just use the mixed returns, when we are mixing two processes with individual path dependence? (See appendix)
# 
# Instead we try doing one MC simulation for `vmr` and one for `pmr`, each with initial value 50, then adding the two simulations to each other.
```

### Version b: Mix of simulations from estimated distribution of returns from individual funds.


```{r eval=params$run_sim}
mc_mmr_b <- mc_simulation(list(fit_vmr, fit_pmr), num_paths = mc_num_paths, num_periods = mc_num_periods, dao = mc_dao)
saveRDS(mc_mmr_b, file="mc_mmr_b.RData")
```


```{r}
mc_mmr_b <- readRDS(file="mc_mmr_b.RData")
```


```{r results='hide',fig.keep='all'}
(mc_mmr_b$mc_plot)()
```


```{r results='hide',fig.keep='all'}
(mc_mmr_b$mc_plot_last_period)()
```


## Convergence

### Max vs sum

Max vs sum plots for the first four moments:

```{r eval=params$run_sim}
mmr_max_sum_plot <- plot_max_sum(fit_mmr, 1e4)
saveRDS(mmr_max_sum_plot, "mmr_max_sum_plot.RData")
```


```{r}
mmr_max_sum_plot <- readRDS(file="mmr_max_sum_plot.RData")
mmr_max_sum_plot
```


### MC

```{r}
mc_mmr_b$mc_conv_plot()
```

### IS

```{r eval=params$run_sim}
is_g_fit_mmr <- is_proposal(
  x_i_fit = fit_mmr,
  x_n_vect = x_n_vect_from_mc_df(mc_mmr_b$mc_df),
  num_paths = mc_num_paths, 
  num_periods = mc_num_periods, 
  obj_func_plot = TRUE,
  init_par = c(2, 0.5),
  method = "L-BFGS-B",
  lower = c(1, 0.1),
  upper = c(3, 0.9)
)
saveRDS(is_g_fit_mmr, "is_g_fit_mmr.RData")
```

```{r}
is_g_fit_mmr <- readRDS(file="is_g_fit_mmr.RData")
```



Parameters
```{r}
is_g_fit_mmr$par
```

Objective function plots

```{r}
is_g_fit_mmr$mean_vect_plot()
```

```{r}
is_g_fit_mmr$sd_vect_plot()
```





```{r eval=params$run_sim}
is_output_mmr <- importance_sampling(
  x_i_fit = fit_mmr, 
  x_n_vect = x_n_vect_from_mc_df(mc_mmr_b$mc_df),
  num_paths = mc_num_paths, 
  num_periods = mc_num_periods, 
  g_n_params = is_g_fit_mmr$par, 
  mode = 1)
saveRDS(is_output_mmr, "is_output_mmr.RData")
```

```{r}
is_output_mmr <- readRDS(file="is_output_mmr.RData")
```



```{r}
is_output_mmr$is_plot()
```



# Mix high risk, 2011 - 2023

## Fit to skew t distribution
```{r}
fit_mhr <- fit_skewed_t(mhr)
```


## QQ Plot
```{r}
(fit_mhr$qqplot)()
```

The qq plot looks good Returns for mixed medium risk portfolios seems to be consistent with a skewed t-distribution.  


## Data vs fit
Let's plot the fit and the observed returns together.  

```{r}
(fit_mhr$fit_plot)()
```

## Estimated distribution
Now lets look at the CDF of the estimated distribution for each 0.1% increment between 0.5% and 99.5% for the estimated distribution:

```{r}
(fit_mhr$dist_plot)()
```

```{r}
(fit_mhr$quantile_plot)()
```

We see that the high risk mix provides a much better upside and smaller downside.

```{r}
(fit_mhr$dens_plot)()
```




## Monte Carlo

### Version a: Simulation from estimated distribution of returns of mix.  
```{r eval=params$run_sim}
mc_mhr_a <- mc_simulation(fit_mhr, num_paths = mc_num_paths, num_periods = mc_num_periods, dao = mc_dao)
saveRDS(mc_mhr_a, file="mc_mhr_a.RData")
```


```{r}
mc_mhr_a  <- readRDS(file="mc_mhr_a.RData")
```


```{r results='hide',fig.keep='all'}
(mc_mhr_a$mc_plot)()
```



```{r results='hide',fig.keep='all'}
(mc_mhr_a$mc_plot_last_period)()
```

```{r}
# NOTE: Can we just use the mixed returns, when we are mixing two processes with individual path dependence? (See appendix)
# 
# Instead we try doing one MC simulation for `vhr` and one for `phr`, each with initial value 50, then adding the two simulations to each other.
```


### Version b: Mix of simulations from estimated distribution of returns from individual funds.

```{r eval=params$run_sim}
mc_mhr_b <- mc_simulation(list(fit_vhr, fit_phr), num_paths = mc_num_paths, num_periods = mc_num_periods, dao = mc_dao)
saveRDS(mc_mhr_b, file="mc_mhr_b.RData")
```


```{r}
mc_mhr_b <- readRDS(file="mc_mhr_b.RData")
```


```{r results='hide',fig.keep='all'}
(mc_mhr_b$mc_plot)()
```


```{r results='hide',fig.keep='all'}
(mc_mhr_b$mc_plot_last_period)()
```


#### Many simulations


```{r}
# mc_mhr_b_many <- mc_simulation(list(fit_vhr, fit_phr), num_paths = 1e6, num_periods = mc_num_periods, dao = mc_dao)
```
1e6 paths:

```{r include=TRUE, echo=TRUE}
# Down-and-out simulation:
# Probability of down-and-out: 0 percent
# 
# Mean portfolio index value after 20 years: 478.339 kr.
# SD of portfolio index value after 20 years: 163.093 kr.
# Min total portfolio index value after 20 years: 2.233 kr.
# Max total portfolio index value after 20 years: 1561.965 kr.
# 
# Share of paths finishing below 100: 0.1181 percent
```



## Convergence

### Max vs sum
Max vs sum plots for the first four moments:

```{r eval=params$run_sim}
mhr_max_sum_plot <- plot_max_sum(fit_mhr, 1e4)
saveRDS(mhr_max_sum_plot, "mhr_max_sum_plot.RData")
```


```{r}
mhr_max_sum_plot <- readRDS(file="mhr_max_sum_plot.RData")
mhr_max_sum_plot
```


### MC

```{r}
mc_mhr_b$mc_conv_plot()
```

### IS

```{r eval=params$run_sim}
is_g_fit_mhr <- is_proposal(
  x_i_fit = fit_mhr,
  x_n_vect = x_n_vect_from_mc_df(mc_mhr_b$mc_df),
  num_paths = mc_num_paths, 
  num_periods = mc_num_periods, 
  obj_func_plot = TRUE,
  init_par = c(1.5, 0.5),
  method = "L-BFGS-B",
  lower = c(1, 0.1),
  upper = c(2, 0.5)
)
saveRDS(is_g_fit_mhr, "is_g_fit_mhr.RData")
```

```{r}
is_g_fit_mhr <- readRDS(file="is_g_fit_mhr.RData")
```


Parameters
```{r}
is_g_fit_mhr$par
```

Objective function plots

```{r}
is_g_fit_mhr$mean_vect_plot()
```

```{r}
is_g_fit_mhr$sd_vect_plot()
```





```{r eval=params$run_sim}
is_output_mhr <- importance_sampling(
  x_i_fit = fit_mhr, 
  x_n_vect = x_n_vect_from_mc_df(mc_mhr_b$mc_df),
  num_paths = mc_num_paths, 
  num_periods = mc_num_periods, 
  g_n_params = is_g_fit_mhr$par, 
  mode = 1)
saveRDS(is_output_mhr, "is_output_mhr.RData")
```

```{r}
is_output_mhr <- readRDS(file="is_output_mhr.RData")
```


```{r}
is_output_mhr$is_plot()
```


# Mix vmr+phr, 2011 - 2023

Log-returns for mix of Velliv medium risk (vm) and PFA high risk (ph):


## Fit to skew t distribution

```{r}
fit_vm_ph <- fit_skewed_t(vm_ph_r)
```


## QQ Plot
```{r}
(fit_vm_ph$qqplot)()
```



## Data vs fit
Let's plot the fit and the observed returns together.  

```{r}
(fit_vm_ph$fit_plot)()
```




## Estimated distribution
Now lets look at the CDF of the estimated distribution for each 0.1% increment between 0.5% and 99.5% for the estimated distribution:

```{r}
(fit_vm_ph$dist_plot)()
```

```{r}
(fit_vm_ph$quantile_plot)()
```



```{r}
(fit_vm_ph$dens_plot)()
```



## Monte Carlo



### Mix of simulations from estimated distribution of returns from individual funds.


```{r eval=params$run_sim}
mc_vm_ph <- mc_simulation(list(fit_vmr, fit_phr), num_paths = mc_num_paths, num_periods = mc_num_periods, dao = mc_dao)
saveRDS(mc_vm_ph, file="mc_vm_ph.RData")
```


```{r}
mc_vm_ph <- readRDS(file="mc_vm_ph.RData")
```


```{r results='hide',fig.keep='all'}
(mc_vm_ph$mc_plot)()
```


```{r results='hide',fig.keep='all'}
(mc_vm_ph$mc_plot_last_period)()
```


## Convergence

### Max vs sum

Max vs sum plots for the first four moments:

```{r eval=params$run_sim}
vm_ph_max_sum_plot <- plot_max_sum(fit_vm_ph, 1e4)
saveRDS(vm_ph_max_sum_plot, "vm_ph_max_sum_plot.RData")
```


```{r}
vm_ph_max_sum_plot <- readRDS(file="vm_ph_max_sum_plot.RData")
vm_ph_max_sum_plot
```


### MC

```{r}
mc_vm_ph$mc_conv_plot()
```

### IS

```{r eval=params$run_sim}
is_g_fit_vm_ph <- is_proposal(
  x_i_fit = fit_vm_ph,
  x_n_vect = x_n_vect_from_mc_df(mc_vm_ph$mc_df),
  num_paths = mc_num_paths, 
  num_periods = mc_num_periods, 
  obj_func_plot = TRUE,
  init_par = c(2, 0.5),
  method = "L-BFGS-B",
  lower = c(1, 0.1),
  upper = c(3, 0.9)
)
saveRDS(is_g_fit_vm_ph, "is_g_fit_vm_ph.RData")
```

```{r}
is_g_fit_vm_ph <- readRDS(file="is_g_fit_vm_ph.RData")
```


Parameters
```{r}
is_g_fit_vm_ph$par
```

Objective function plots

```{r}
is_g_fit_vm_ph$mean_vect_plot()
```

```{r}
is_g_fit_vm_ph$sd_vect_plot()
```





```{r eval=params$run_sim}
is_output_vm_ph <- importance_sampling(
  x_i_fit = fit_vm_ph, 
  x_n_vect = x_n_vect_from_mc_df(mc_vm_ph$mc_df),
  num_paths = mc_num_paths, 
  num_periods = mc_num_periods, 
  g_n_params = is_g_fit_vm_ph$par, 
  mode = 1)
saveRDS(is_output_vm_ph, "is_output_vm_ph.RData")
```

```{r}
is_output_vm_ph <- readRDS(file="is_output_vm_ph.RData")
```


```{r}
is_output_vm_ph$is_plot()
```



# Mix vhr+pmr, 2011 - 2023

Log-returns for mix of Velliv high risk (vh) and PFA medium risk (pm):


## Fit to skew t distribution

```{r}
fit_vh_pm <- fit_skewed_t(vh_pm_r)
```


## QQ Plot
```{r}
(fit_vh_pm$qqplot)()
```




## Data vs fit
Let's plot the fit and the observed returns together.  

```{r}
(fit_vh_pm$fit_plot)()
```



## Estimated distribution
Now lets look at the CDF of the estimated distribution for each 0.1% increment between 0.5% and 99.5% for the estimated distribution:

```{r}
(fit_vh_pm$dist_plot)()
```

```{r}
(fit_vh_pm$quantile_plot)()
```


```{r}
(fit_vh_pm$dens_plot)()
```



## Monte Carlo



### Mix of simulations from estimated distribution of returns from individual funds.


```{r eval=params$run_sim}
mc_vh_pm <- mc_simulation(list(fit_vhr, fit_pmr), num_paths = mc_num_paths, num_periods = mc_num_periods, dao = mc_dao)
saveRDS(mc_vh_pm, file="mc_vh_pm.RData")
```


```{r}
mc_vh_pm <- readRDS(file="mc_vh_pm.RData")
```


```{r results='hide',fig.keep='all'}
(mc_vh_pm$mc_plot)()
```


```{r results='hide',fig.keep='all'}
(mc_vh_pm$mc_plot_last_period)()
```


## Convergence

### Max vs sum

Max vs sum plots for the first four moments:

```{r eval=params$run_sim}
vh_pm_max_sum_plot <- plot_max_sum(fit_vh_pm, 1e4)
saveRDS(vh_pm_max_sum_plot, "vh_pm_max_sum_plot.RData")
```


```{r}
vh_pm_max_sum_plot <- readRDS(file="vh_pm_max_sum_plot.RData")
vh_pm_max_sum_plot
```


### MC

```{r}
mc_vh_pm$mc_conv_plot()
```

### IS

```{r eval=params$run_sim}
is_g_fit_vh_pm <- is_proposal(
  x_i_fit = fit_vh_pm,
  x_n_vect = x_n_vect_from_mc_df(mc_vh_pm$mc_df),
  num_paths = mc_num_paths, 
  num_periods = mc_num_periods, 
  obj_func_plot = TRUE,
  init_par = c(2, 0.5),
  method = "L-BFGS-B",
  lower = c(1, 0.1),
  upper = c(3, 0.9)
)
saveRDS(is_g_fit_vh_pm, "is_g_fit_vh_pm.RData")
```

```{r}
is_g_fit_vh_pm <- readRDS(file="is_g_fit_vh_pm.RData")
```


Parameters
```{r}
is_g_fit_vh_pm$par
```

Objective function plots

```{r}
is_g_fit_vh_pm$mean_vect_plot()
```

```{r}
is_g_fit_vh_pm$sd_vect_plot()
```





```{r eval=params$run_sim}
is_output_vh_pm <- importance_sampling(
  x_i_fit = fit_vh_pm, 
  x_n_vect = x_n_vect_from_mc_df(mc_vh_pm$mc_df),
  num_paths = mc_num_paths, 
  num_periods = mc_num_periods, 
  g_n_params = is_g_fit_vh_pm$par, 
  mode = 1)
saveRDS(is_output_vh_pm, "is_output_vh_pm.RData")
```

```{r}
is_output_vh_pm <- readRDS(file="is_output_vh_pm.RData")
```


```{r}
is_output_vh_pm$is_plot()
```



# Compare pension plans

## Risk of max loss
Risk of max loss of x percent for a single period (year).  
x values are row names.  

```{r}
# Use "max" for losses
percent_vals = c(0, 5, 10, 25, 50, 90, 99)
risk_percentiles_df <-  data.frame(
      #Percent = percent_vals,
      Vel_m = risk_percentiles(fit_vmr$quantile_data, percent_vals, "max"),
      Vel_ml = risk_percentiles(fit_vmrl$quantile_data, percent_vals, "max"),
      Vel_h = risk_percentiles(fit_vhr$quantile_data, percent_vals, "max"),
      PFA_m = risk_percentiles(fit_pmr$quantile_data, percent_vals, "max"),
      PFA_h = risk_percentiles(fit_phr$quantile_data, percent_vals, "max"),
      mix_m = risk_percentiles(fit_mmr$quantile_data, percent_vals, "max"),
      mix_h = risk_percentiles(fit_mhr$quantile_data, percent_vals, "max"),
      vm_ph = risk_percentiles(fit_vm_ph$quantile_data, percent_vals, "max"),
      vh_pm = risk_percentiles(fit_vh_pm$quantile_data, percent_vals, "max")
)

rownames(risk_percentiles_df) <- as.character(percent_vals)
```

```{r}
knitr::kable(risk_percentiles_df, digits = 3)
```


### Worst ranking for loss percentiles

```{r}
knitr::kable(rank_summary(risk_percentiles_df, sorting = rep("hi", length(percent_vals))), digits = 3)
```



## Chance of min gains
Chance of min gains of x percent for a single period (year).  
x values are row names.

```{r}
# Use "min" for gains
percent_vals = c(0, 5, 10, 25, 50, 100)
gain_percentiles_df <-  data.frame(
      #Percent = percent_vals,
      Velliv_m = risk_percentiles(fit_vmr$quantile_data, percent_vals, "min"),
      Velliv_m_l = risk_percentiles(fit_vmrl$quantile_data, percent_vals, "min"),
      Velliv_h = risk_percentiles(fit_vhr$quantile_data, percent_vals, "min"),
      PFA_m = risk_percentiles(fit_pmr$quantile_data, percent_vals, "min"),
      PFA_h = risk_percentiles(fit_phr$quantile_data, percent_vals, "min"),
      mix_m = risk_percentiles(fit_mmr$quantile_data, percent_vals, "min"),
      mix_h = risk_percentiles(fit_mhr$quantile_data, percent_vals, "min"),
      vm_ph = risk_percentiles(fit_vm_ph$quantile_data, percent_vals, "min"),
      vh_pm = risk_percentiles(fit_vh_pm$quantile_data, percent_vals, "min")
)

rownames(gain_percentiles_df) <- as.character(percent_vals)
```


```{r}
knitr::kable(gain_percentiles_df, digits = 3)
```

### Best ranking for gains percentiles

```{r}
knitr::kable(rank_summary(gain_percentiles_df, sorting = rep("hi", length(percent_vals))), digits = 3)
```




## MC risk percentiles

Risk of loss from first to last period.  

`_m` is medium.  
`_h`  is high.  

`a` is simulation from estimated distribution of returns of mix.  
`b` is mix of simulations from estimated distribution of returns from individual 
funds.

`l` for "long", going back to 2007.

```{r}
# Use "max" for losses
percent_vals = c(0, 5, 10, 25, 50, 90, 99)
mc_loss_percentiles_df <-  data.frame(
      #Percent = percent_vals,
      Vel_m = risk_percentiles(log(unlist(mc_vmr$mc_df[20, ])/100), percent_vals, "max"),
      Vel_ml = risk_percentiles(log(unlist(mc_vmrl$mc_df[20, ])/100), percent_vals, "max"),
      Vel_h = risk_percentiles(log(unlist(mc_vhr$mc_df[20, ])/100), percent_vals, "max"),
      PFA_m = risk_percentiles(log(unlist(mc_pmr$mc_df[20, ])/100), percent_vals, "max"),
      PFA_h = risk_percentiles(log(unlist(mc_phr$mc_df[20, ])/100), percent_vals, "max"),
      mix_ma = risk_percentiles(log(unlist(mc_mmr_a$mc_df[20, ])/100), percent_vals, "max"),
      mix_ha = risk_percentiles(log(unlist(mc_mhr_a$mc_df[20, ])/100), percent_vals, "max"),
      mix_mb = risk_percentiles(log(unlist(mc_mmr_b$mc_df[20, ])/100), percent_vals, "max"),
      mix_hb = risk_percentiles(log(unlist(mc_mhr_b$mc_df[20, ])/100), percent_vals, "max"),
      vm_ph = risk_percentiles(log(unlist(mc_vm_ph$mc_df[20, ])/100), percent_vals, "max"),
      vh_pm = risk_percentiles(log(unlist(mc_vh_pm$mc_df[20, ])/100), percent_vals, "max")
)

rownames(mc_loss_percentiles_df) <- as.character(percent_vals)
```

```{r}
knitr::kable(mc_loss_percentiles_df, digits = 3)
```


1e6 simulation paths of `mhr_b`:
```{r}
# percent_vals = c(0, 5, 10, 25, 50, 90, 99)
# mc_mhr_b_many_mc_loss_percentiles <-
#   data.frame(
#     percentiles = percent_vals,
#     prob=risk_percentiles(log(unlist(mc_mhr_b_many$mc_df[20, ])/100), percent_vals, "max")
#   )
# 
# t(mc_mhr_b_many_mc_loss_percentiles)
```

```{r}
# knitr::kable(t(mc_mhr_b_many_mc_loss_percentiles), digits = 3)
```
```{r}
percent_vals = c(0, 5, 10, 25, 50, 90, 99)
mc_mhr_b_many_mc_loss_percentiles <- 
  data.frame(
    prob_pct = c(0.118, 0.095, 0.076, 0.036, 8e-03, 0, 0)
  )
rownames(mc_mhr_b_many_mc_loss_percentiles) <- as.character(percent_vals)
```

```{r}
knitr::kable(t(mc_mhr_b_many_mc_loss_percentiles), digits = 3)
```


### Worst ranking for MC loss percentiles

```{r}
knitr::kable(rank_summary(mc_loss_percentiles_df, sorting = rep("hi", length(percent_vals))), digits = 3)
```





## MC gains percentiles

Chance of gains from first to last period.  
`_a` is simulation from estimated distribution of returns of mix.  
`_b` is mix of simulations from estimated distribution of returns from individual 
funds.

```{r}
# Use "min" for gains
percent_vals = c(0, 5, 10, 25, 50, 100, 200, 300, 400, 500, 1000)
mc_gain_percentiles_df <-  data.frame(
      #Percent = percent_vals,
      Vel_m = risk_percentiles(log(unlist(mc_vmr$mc_df[20, ])/100), percent_vals, "min"),
      Vel_ml = risk_percentiles(log(unlist(mc_vmrl$mc_df[20, ])/100), percent_vals, "min"),
      Vel_h = risk_percentiles(log(unlist(mc_vhr$mc_df[20, ])/100), percent_vals, "min"),
      PFA_m = risk_percentiles(log(unlist(mc_pmr$mc_df[20, ])/100), percent_vals, "min"),
      PFA_h = risk_percentiles(log(unlist(mc_phr$mc_df[20, ])/100), percent_vals, "min"),
      mix_ma = risk_percentiles(log(unlist(mc_mmr_a$mc_df[20, ])/100), percent_vals, "min"),
      mix_ha = risk_percentiles(log(unlist(mc_mhr_a$mc_df[20, ])/100), percent_vals, "min"),
      mix_mb = risk_percentiles(log(unlist(mc_mmr_b$mc_df[20, ])/100), percent_vals, "min"),
      mix_hb = risk_percentiles(log(unlist(mc_mhr_b$mc_df[20, ])/100), percent_vals, "min"),
      vm_ph = risk_percentiles(log(unlist(mc_vm_ph$mc_df[20, ])/100), percent_vals, "min"),
      vh_pm = risk_percentiles(log(unlist(mc_vh_pm$mc_df[20, ])/100), percent_vals, "min")
)

rownames(mc_gain_percentiles_df) <- as.character(percent_vals)
```

```{r}
knitr::kable(mc_gain_percentiles_df, digits = 3)
```


1e6 simulation paths of `mhr_b`: 

```{r}
# percent_vals = c(0, 5, 10, 25, 50, 100, 200, 300, 400, 500, 1000)
# mc_mhr_b_many_mc_loss_percentiles <-
#   data.frame(
#     percentiles = percent_vals,
#     prob=risk_percentiles(log(unlist(mc_mhr_b_many$mc_df[20, ])/100), percent_vals, "min")
#   )
# t(mc_mhr_b_many_mc_loss_percentiles)
```

```{r}
percent_vals = c(0, 5, 10, 25, 50, 100, 200, 300, 400, 500, 1000)
mc_mhr_b_many_mc_gains_percentiles <- 
  data.frame(
    prob = c(99.8819, 99.8536, 99.824, 99.6861, 99.3009, 97.5134, 86.9115, 65.992, 41.486, 21.6928, 8.65e-02)
  )

rownames(mc_mhr_b_many_mc_gains_percentiles) <- as.character(percent_vals)
```

```{r}
knitr::kable(t(mc_mhr_b_many_mc_gains_percentiles), digits = 3)
```


### Best ranking for MC gains percentiles

```{r}
knitr::kable(rank_summary(mc_gain_percentiles_df[1:6, ], sorting = rep("hi", length(percent_vals))), digits = 3)
```

```{r}
knitr::kable(rank_summary(mc_gain_percentiles_df[7:11, ], sorting = rep("hi", length(percent_vals))), digits = 3)
```


## Summary statistics  

### Fit summary
Summary for fit of log returns to an F-S skew standardized Student-t distribution.  
`m`  is the location parameter.  
`s` is the scale parameter.  
`nu` is the estimated degrees of freedom, or shape parameter.  
`xi` is the estimated skewness parameter.  

```{r}
fit_summary <- data.frame(
  Vel_m = c(fit_vmr$m, fit_vmr$s, fit_vmr$nu, fit_vmr$xi, fit_vmr$r_squared),
  Vel_ml = c(fit_vmrl$m, fit_vmrl$s, fit_vmrl$nu, fit_vmrl$xi, fit_vmrl$r_squared),
  Vel_h = c(fit_vhr$m, fit_vhr$s, fit_vhr$nu, fit_vhr$xi, fit_vhr$r_squared),
  PFA_m = c(fit_pmr$m, fit_pmr$s, fit_pmr$nu, fit_pmr$xi, fit_pmr$r_squared),
  PFA_h = c(fit_phr$m, fit_phr$s, fit_phr$nu, fit_phr$xi, fit_phr$r_squared),
  mix_m = c(fit_mmr$m, fit_mmr$s, fit_mmr$nu, fit_mmr$xi, fit_mmr$r_squared),
  mix_h = c(fit_mhr$m, fit_mhr$s, fit_mhr$nu, fit_mhr$xi, fit_mhr$r_squared),
  vm_ph = c(fit_vm_ph$m, fit_vm_ph$s, fit_vm_ph$nu, fit_vm_ph$xi, fit_vm_ph$r_squared),
  vh_pm = c(fit_vh_pm$m, fit_vh_pm$s, fit_vh_pm$nu, fit_vh_pm$xi, fit_vh_pm$r_squared)
)

rownames(fit_summary) <- c("m", "s", "nu", "xi", "R^2")
```

```{r}
knitr::kable(fit_summary, digits = 3)
```

#### Fit statistics ranking

```{r}
knitr::kable(rank_summary(fit_summary[c(1, 2, 5), ], sorting = c("hi", "lo", "hi")), digits = 3)
```



### Monte Carlo simulations summary

Monte Carlo simulations of portfolio index values (currency values).  
Statistics are given for the final state of all paths.  
Probability of down-and_out is calculated as the share of paths that reach 0 at
some point. All subsequent values for a path are set to 0, if the path reaches
at any point.  
0 is defined as any value below a threshold.    
`dai_pct` (for down-and-in) is the probability of losing money. This is calculated as the 
share of paths finishing below index 100.  

```{r}
cat("Number of paths:", mc_num_paths, "\n")
```


```{r}
mc_summary <- data.frame(
  Vel_m = c(mc_vmr$mc_m, mc_vmr$mc_s, mc_vmr$mc_min, mc_vmr$mc_max, mc_vmr$dao_probability_percent, mc_vmr$percent_losing_paths),
  Vel_ml = c(mc_vmrl$mc_m, mc_vmrl$mc_s, mc_vmrl$mc_min, mc_vmrl$mc_max, mc_vmrl$dao_probability_percent, mc_vmrl$percent_losing_paths),
  Vel_h = c(mc_vhr$mc_m, mc_vhr$mc_s, mc_vhr$mc_min, mc_vhr$mc_max, mc_vhr$dao_probability_percent, mc_vhr$percent_losing_paths),
  PFA_m = c(mc_pmr$mc_m, mc_pmr$mc_s, mc_pmr$mc_min, mc_pmr$mc_max, mc_pmr$dao_probability_percent, mc_pmr$percent_losing_paths),
  PFA_h = c(mc_phr$mc_m, mc_phr$mc_s, mc_phr$mc_min, mc_phr$mc_max, mc_phr$dao_probability_percent, mc_phr$percent_losing_paths),
  mix_ma = c(mc_mmr_a$mc_m, mc_mmr_a$mc_s, mc_mmr_a$mc_min, mc_mmr_a$mc_max, mc_mmr_a$dao_probability_percent, mc_mmr_a$percent_losing_paths),
  mix_mb = c(mc_mmr_b$mc_m, mc_mmr_b$mc_s, mc_mmr_b$mc_min, mc_mmr_b$mc_max, mc_mmr_b$dao_probability_percent, mc_mmr_b$percent_losing_paths),
  mix_ha = c(mc_mhr_a$mc_m, mc_mhr_a$mc_s, mc_mhr_a$mc_min, mc_mhr_a$mc_max, mc_mhr_a$dao_probability_percent, mc_mhr_a$percent_losing_paths),
  mix_hb = c(mc_mhr_b$mc_m, mc_mhr_b$mc_s, mc_mhr_b$mc_min, mc_mhr_b$mc_max, mc_mhr_b$dao_probability_percent, mc_mhr_b$percent_losing_paths),
  vm_ph = c(mc_vm_ph$mc_m, mc_vm_ph$mc_s, mc_vm_ph$mc_min, mc_vm_ph$mc_max, mc_vm_ph$dao_probability_percent, mc_vm_ph$percent_losing_paths),
  vh_pm = c(mc_vh_pm$mc_m, mc_vh_pm$mc_s, mc_vh_pm$mc_min, mc_vh_pm$mc_max, mc_vh_pm$dao_probability_percent, mc_vh_pm$percent_losing_paths)
)

rownames(mc_summary) <- c("mc_m", "mc_s", "mc_min", "mc_max", "dao_pct", "dai_pct")
```


```{r}
knitr::kable(mc_summary, digits = 2)
```


#### Ranking

```{r}
knitr::kable(rank_summary(mc_summary, sorting = c("hi", "lo", "hi", "hi", "lo", "lo")), digits = 2)
```

# Compare Gaussian and skewed t-distribution fits

## Gaussian fits

```{r}
fits_norm <- as.data.frame(lapply(
  data_df,
  function(x) {
    fit_g <- fit_gauss(x)
    c(fit_g$par[1], fit_g$par[2])
  }
))
rownames(fits_norm) <- c("m", "s")
```

```{r}
knitr::kable(fits_norm, digits = 3)
```


```{r fig.width=9, fig.height=12}
plot_norm_cdf <- function(x, m, s, minx, maxx, i) {
  plot(seq(from = minx, to = maxx, length.out = 100), pnorm(seq(from = minx, to = maxx, length.out = 100), m, s), pch = 16, cex = 0.3, type = "l", main = names(data_df)[i], xlab = "p", ylab = "log-return")
  abline(v = c(min(x), max(x)), col = c("red", "green"))
  abline(h = c(pnorm(min(x), m, s), pnorm(max(x), m,s)), col = c("red", "green"))
}

par(mfrow = c(4, 2))
for(i in 1:ncol(data_df)) {
  m <- fits_norm[, i][1]
  s <- fits_norm[, i][2]
  plot_norm_cdf(data_df[, i], m, s, -0.3, 0.3, i)
}
par(mfrow = c(1, 1))
```

### Gaussian QQ plots

```{r fig.width=9, fig.height=12}
plot_norm_qq <- function(x, m, s, i) {
  qqnorm((x - m) / s, main = names(data_df)[i])
  qqline((x - m) / s)
  abline(0, 1, col = "red")
  legend("bottomright", legend = c("fit", "y=x"), col = c("black", "red"), lty = 1)
}

par(mfrow = c(4, 2))
for(i in 1:ncol(data_df)) {
  m <- fits_norm[, i][1]
  s <- fits_norm[, i][2]
  plot_norm_qq(data_df[, i], m, s, i)
}
par(mfrow = c(1, 1))
```

### Gaussian vs skewed t

```{r}
# norm_probs <- as.data.frame(lapply(data_df, function(x) {
#   c(
#     pnorm(min(x), mean(x), sd(x)) * 100, 
#     (1 - pnorm(max(x), mean(x), sd(x))) * 100
#   )
# }))[, c(1, 4,2, 5, 3, 6)]
# rownames(norm_probs) <- c("P(X_min)", "P(X_max)")

make_extreme_probs <- function(data_df) {
  extreme_probs <- data.frame(matrix(4 * ncol(data_df), 4, ncol(data_df)))
  for(i in 1:ncol(data_df)) {
    x <- data_df[, i]
    extreme_probs[, i] <- c(
      pnorm(min(x), fits_norm["m", i], fits_norm["s", i]) * 100, 
      (1 - pnorm(max(x), fits_norm["m", i], fits_norm["s", i])) * 100,
      psstd(min(x), fit_summary[, -2]["m", i], fit_summary[, -2]["s", i], fit_summary[, -2]["nu", i], fit_summary[, -2]["xi", i]) * 100, 
      (1 - psstd(max(x), fit_summary[, -2]["m", i], fit_summary[, -2]["s", i], fit_summary[, -2]["nu", i], fit_summary[, -2]["xi", i])) * 100
    )
  }
  extreme_probs
}
extreme_probs <- make_extreme_probs(data_df)
colnames(extreme_probs) <- colnames(data_df)
rownames(extreme_probs) <- c("P_norm(X_min)", "P_norm(X_max)", "P_t(X_min)", "P_t(X_max)")


avg_yrs_btw_events <- 100/extreme_probs
rownames(avg_yrs_btw_events) <- c("norm: avg yrs btw min", "norm: avg yrs btw max", "t: avg yrs btw min", "t: avg yrs btw max")
```

Probability in percent that the smallest and largest (respectively) observed return for each fund was generated by a normal distribution:
```{r}
knitr::kable(extreme_probs, digits = 3)
```

Average number of years between min or max events (respectively):
```{r}
knitr::kable(avg_yrs_btw_events, digits = 3)
```


# Comments

(Ignoring `mhr_a`...)

`mhr` has some nice properties:  
- It has a relatively high `nu` value of 90, which means it is tending more towards exponential tails than polynomial tails. All other funds have `nu` values close to 3, except `phr` which is even worse at close to 2. (Note that for a Gaussian, `nu` is infinite.)  
- It has the lowest losing percentage of all simulations, which is better than 1/6 that of `phr`.  
- It has a DAO percentage of 0, which is the same as `mmr`, and less than `phr`.  
- Only `phr` has a higher `mc_m`.  
- It has a smaller `mc_s` than the individual components, `vhr` and `phr`.  
- It has the highest `xi` of all fits, suggesting less left skewness. Density plots for `vmr`, `phr` and `mmr` have an extremely sharp drop, as if an upward limiter has been applied, which corresponds to extremely low `xi` values. The density plot for `mhr` is by far the most symmetrical of all the fits. As seen in the section "Compare Gaussian and skewed t-distribution fits", the other skewed t-distribution fits don't capture the max observed returns at all.    
- Only `mmr` has as higher `mc_min`. However, that of `mmr` is 18 times higher with 62, so `mmr` is a clear winner here.  
- Naturally, it has a `mc_max` smaller than the individual components, `vhr` and `phr`, but ca. 1.5 times higher then `mmr`.  
- All the first 4 moments converge nicely. For all other fits, the 4th moment doesn't seem to converge.  


Taleb, Statistical Consequences Of Fat Tails, p. 97:  
"the variance of a finite variance random variable with tail exponent $< 4$ will be infinite".

And p. 363:  
"The hedging errors for an option portfolio (under a daily revision regime) over 3000 days, un- der a constant volatility Student T with tail exponent $\alpha = 3$. Technically the errors should not converge in finite time as their distribution has infinite variance."

- Importance Sampling seems to converge to a lower level than Monte Carlo does. Is that because IS catches more observations in the lower tail? Supporting this thesis is that MC for `mhr` with `1e4` paths gives a mean of 520, while `1e6` paths gives a mean of 478 (see under "Many simulations").  
- Note: QQ lines by design pass through 1st and 3rd quantiles. They are not trendlines in the sense of linear regression.  




# Appendix
## Average of returns vs returns of average

### Math

$$\text{Avg. of returns} := \dfrac{ \left(\dfrac{x_t}{x_{t-1}} + \dfrac{y_t}{y_{t-1}} \right) }{2}$$
$$\text{Returns of avg.} := \left(\dfrac{ x_t + y_t }{2}\right) \Big/ \left(\dfrac{ x_{t-1} + y_{t-1} }{2}\right) \equiv \dfrac{ x_t + y_t }{ x_{t-1} + y_{t-1}}$$

For which $x_1$ and $y_1$ are $\text{Avg. of returns} = \text{Returns of avg.}$?

$$\dfrac{ \left(\dfrac{x_t}{x_{t-1}} + \dfrac{y_t}{y_{t-1}} \right) }{2} = \dfrac{ x_t + y_t }{ x_{t-1} + y_{t-1}}$$

$$\dfrac{x_t}{x_{t-1}} + \dfrac{y_t}{y_{t-1}} = 2 \dfrac{ x_t + y_t }{ x_{t-1} + y_{t-1}}$$

$$(x_{t-1} + y_{t-1}) x_t y_{t-1} + (x_{t-1} + y_{t-1}) x_{t-1} y_t = 2 (x_{t-1}y_{t-1}x_t + x_{t-1}y_{t-1}y_t)$$


$$(x_{t-1}x_1y_{t-1} + y_{t-1}x_ty_{t-1}) + (x_{t-1}x_{t-1}y_t + x_{t-1}y_{t-1}y_t) = 2(x_{t-1}y_{t-1}x_t + x_{t-1}y_{t-1}y_t)$$
This is not generally true, but true if for instance $x_{t-1} = y_{t-1}$.




### Example

```{r}
x0 <- 100
y0 <- 200
Rx <- 0.5
Ry <- 1.5
```

Definition: `R = 1+r`

```{r}
cat(paste0("Let x_0 be ", x0, ".\n"))
cat(paste0("Let y_0 be ", y0, ".\n"))
cat("So the initial value of the pf is", x0 + y0, ".\n")
cat("\n")
cat(paste0("Let R_x be ", Rx, ".\n"))
cat(paste0("Let R_y be ", Ry, ".\n"))
```

Then, 
```{r}
cat(paste0("x_1 is R_x * x_0 = ", Rx * x0, ".\n"))
cat(paste0("y_1 is R_y * y_0 = ", Ry * y0, ".\n"))
```

Average of returns:  
```{r}
cat("0.5 * (R_x + R_y) =", 0.5 * (Rx + Ry), "\n")
```

So here the value of the pf at t=1 should be unchanged from t=0:  
```{r}
cat("(x_0 + y_0) * 0.5 * (R_x + R_y) =", (x0 + y0) * 0.5 * (Rx + Ry), "\n")
```

But this is clearly not the case:  
```{r}
cat("0.5 * (x_1 + y_1) = 0.5 * (R_x * x_0 + R_y * y_0) =", 0.5 * (Rx * x0 + Ry * y0), "\n")
```

Therefore we should take returns of average, not average of returns!  

Let's take the average of log returns instead:  
```{r}
cat("0.5 * (log(R_x) + log(R_y)) =", 0.5 * (log(Rx) + log(Ry)), "\n")
```

We now get:  
```{r}
cat("(x_0 + y_0) * exp(0.5 * (log(Rx) + log(Ry))) =", (x0 + y0) * exp(0.5 * (log(Rx) + log(Ry))), "\n")
```

So taking the average of log returns doesn't work either.



## Simulation of mix vs mix of simulations
Test if a simulation of a mix (average) of two returns series has the same distribution as a mix of two simulated returns series.


```{r}
mc_sim <- function(
    num_runs = 1, 
    num_paths = 1000, 
    num_periods = 20,
    m_a = 0,
    s_a = 0.4,
    m_b = 10,
    s_b = 3
    ) {
  
  data_x <- rnorm(num_periods, m_a, s_a)
  data_y <- rnorm(num_periods, m_b, s_b)
  
  m_data_x <- mean(data_x)
  s_data_x <- sd(data_x)
  m_data_y <- mean(data_y)
  s_data_y <- sd(data_y)
  
  cat("m(data_x):", m_data_x, "\n")
  cat("s(data_x):", s_data_x, "\n")
  cat("m(data_y):", m_data_y, "\n")
  cat("s(data_y):", s_data_y, "\n")
  cat("\n")
  
  m_data_xy <- mean(0.5 * data_x + 0.5 * data_y)
  s_data_xy <- sd(0.5 * data_x + 0.5 * data_y)
  
  cat("m(data_x + data_y):", m_data_xy, "\n")
  cat("s(data_x + data_y):", s_data_xy, "\n")
  cat("\n")
  
  run_sim <- function(num_runs) {
    df <- data.frame(
      m_a = rep(0, num_runs), m_b = rep(0, num_runs), 
      s_a = rep(0, num_runs), s_b = rep(0, num_runs)
    )
    for(j in 1:num_runs) {
      sim_x <- rep(0, num_paths)
      sim_y <- rep(0, num_paths)
      sim_xy <- rep(0, num_paths)
      for(i in 1:num_paths) {
        sim_x[i] <- sum(rnorm(num_periods, m_data_x, s_data_x))
        sim_y[i] <- sum(rnorm(num_periods, m_data_y, s_data_y))
        sim_xy[i] <- sum(rnorm(num_periods, m_data_xy, s_data_xy))
      }

      df$m_a[j] <-  mean(0.5 * sim_x + 0.5 * sim_y)
      df$m_b[j] <-  mean(sim_xy)
      df$s_a[j] <-  sd(0.5 * sim_x + 0.5 * sim_y)
      df$s_b[j] <-  sd(sim_xy)
    }
    df
  }
  
  run_sim(num_runs)
}
```

```{r}
mc_sim_df <- mc_sim(
  num_runs = 10, 
  num_paths = 1000, 
  num_periods = 20,
  m_a = 0,
  s_a = 0.4,
  m_b = 10,
  s_b = 3
)
```

m and s of final state of all paths.  
`_a` is mix of simulated returns.  
`_b` is simulated mixed returns.  

```{r}
knitr::kable(mc_sim_df, digits = 3)
```

```{r}
summary(mc_sim_df)
```

`_a` and `_b` are very close to equal.  
We attribute the differences to differences in estimating the distributions in 
version a and b.  


The final state is independent of the order of the preceding steps:  

```{r}
vect1 <- c(rnorm(100))
vect2 <- c(sample(vect1, 100))
vect3 <- c(sample(vect1, 100))
path1 <- c(0, cumsum(vect1))
path2 <- c(0, cumsum(vect2))
path3 <- c(0, cumsum(vect3))
plot(path1, type = "l", col = "blue", 
     ylim = c(
       min(c(path1, path2, path3)), 
       max(c(path1, path2, path3))
     )
)
lines(path2, col = "red")
lines(path3, col = "green")
```


So does the order of the steps in the two processes matter, when mixing simulated returns?  

```{r}
vect1a <- c(rnorm(100, 0.05, 0.06))
vect1b <- c(sample(vect1a, 100))
vect2a <- c(rnorm(100, 0.05, 0.06))
vect2b <- c(sample(vect2a, 100))

path1a <- 100 * c(1, cumprod(1 + vect1a))
path1b <- 100 * c(1, cumprod(1 + vect1b))
path2a <- 100 * c(1, cumprod(1 + vect2a))
path2b <- 100 * c(1, cumprod(1 + vect2b))

mix_path_a <- 0.5 * path1a + 0.5 * path2a
mix_path_b <- 0.5 * path1b + 0.5 * path2b

plot(path1a, type = "l", lty = 1, col = "blue", 
     ylim = c(
       min(c(path1a, path1b, path2a, path2b)), 
       max(c(path1a, path1b, path2a, path2b))
     )
)
lines(path1b, lty = 2, col = "blue")
lines(path2a, lty = 1, col = "red")
lines(path2b, lty = 2, col = "red")
```

```{r}
plot(mix_path_a, type = "l", lty = 1, col = "blue", 
     ylim = c(
       min(c(mix_path_a, mix_path_b)), 
       max(c(mix_path_a, mix_path_b))
     )
)
lines(mix_path_b, lty = 2, col = "blue")
```

The order of steps in the individual paths do not matter, because the mix of simulated paths is a sum of a sum, so the order of terms doesn't affect the sum. If there is variation it is because the sets preceding steps are not the same. For instance, the steps between step 1 and 60 in the plot above are not the same for the two lines.

Recall,
$$\text{Var}(aX+bY) = a^2 \text{Var}(X) + b^2 \text{Var}(Y) + 2ab \text{Cov}(a, b)$$

```{r include=TRUE, echo=TRUE}
var(0.5 * vhr + 0.5 * phr)
0.5^2 * var(vhr) + 0.5^2 * var(phr) + 2 * 0.5 * 0.5 * cov(vhr, phr)
```


Our distribution estimate is based on 13 observations. Is that enough for a robust estimate?
What if we suddenly hit a year like 2008? How would that affect our estimate?  
Let's try to include the Velliv data from 2007-2010.  
We do this by sampling 13 observations from `vmrl`.  

```{r}
n <- 50
test_df <- data.frame(m = rep(0, n), s = rep(0, n))
for(i in 1:n) {
  vmrl_smp <- sample(vmrl, 13)
  test_df$m[i] <- mean(0.5 * vmrl_smp + 0.5 * phr)
  test_df$s[i] <- sd(0.5 * vmrl_smp + 0.5 * phr)
}
summary(test_df)
```


## The meaning of `xi`

The fit for `mhr` has the highest `xi` value of all. This suggests right-skew:

```{r}
plot((-20000:20000)/1000, dsstd((-20000:20000)/1000, 0, 7, 5, 1), xlim = c(-20, 20), ylim = c(0, 0.1), type = "l", xlab = "value", ylab = "likelihood", main = "Skew t-distribution density")
lines((-20000:20000)/1000, dsstd((-20000:20000)/1000, 0, 7, 5, 0.5), col="red")
lines((-20000:20000)/1000, dsstd((-20000:20000)/1000, 0, 7, 5, 2), col="blue")
legend("topright", c("xi=1", "xi=0.5", "xi=2"), col = c("black", "red", "blue"), lty = 1)
```


## Max vs sum plot


If the Law Of Large Numbers holds true,
$$\dfrac{\max (X_1^p, ..., X^p)}{\sum_{i=1}^n X_i^p} \rightarrow 0$$
for $n \rightarrow \infty$.

If not, $X$ doesn't have a $p$'th moment.

See Taleb: The Statistical Consequences Of Fat Tails, p. 192


```{r include=FALSE}
## For knitting "pensionsafkast_IS.Rmd":
saveRDS(fit_mhr, file="fit_mhr.RData")
```

```{bash eval=FALSE, include=FALSE}
## Convert md to pdf
## Run manually:
pandoc pensionsafkast.md -o pensionsafkast.pdf
```