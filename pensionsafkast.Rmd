---
title: "Pension returns analysis"
header-includes:
   - \usepackage[default]{sourcesanspro}
   - \usepackage[T1]{fontenc}
   - \usepackage[fontsize=8pt]{scrextend}
mainfont: SourceSansPro
output: 
  html_document:
    toc: true
    toc_depth: 3
  # pdf_document:
  #   toc: true
  #   toc_depth: 3
#fontsize: 10pt # for pdf. Limited to 10pt, 11pt and 12pt. Else use scrextend.
date: "`r format(Sys.time(), '%H:%M %d %B %Y')`"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, warning=FALSE, message=FALSE)
```

```{r include=FALSE}
library("fGarch")
library(RColorBrewer)
library(scales)
```

```{r}
mc_num_paths <- 5000
mc_num_periods <- 20
mc_dao <- TRUE
```



Fit log returns to F-S skew standardized Student-t distribution.  
`m`  is the location parameter.  
`s` is the scale parameter.  
`nu` is the estimated degrees of freedom.  
`xi` is the estimated shape parameter.  
```{r}

## Calculate risk percentiles.
## data can be a data frame or a list of data frames.
# risk_percentiles <- function(data, percent, max_or_min) {
#   init_capital = 100
#   if(!is.list(data)) {
#     init_capital = 100
#   } else {
#     init_capital <- init_capital/length(data)
#     vect <- data[[1]]
#     if(length(data) > 1) {
#       for(i in 2:length(data)) {
#         vect <- vect + data[[i]]
#       }
#     }
#     data <- vect
#   }
#   if(max_or_min == "max"){
#       init_capital * (length(which(data < (100 - percent)/100)) + 0.5) / 1000
#     } else {
#       init_capital * (length(which(data > (100 + percent)/100)) + 0.5) / 1000
#     }
# }

## dist_data is given as log returns.
risk_percentiles <- function(dist_data, percent, max_or_min) {
  num_points <- length(dist_data)
  log_returns <- sort(dist_data)
  if(max_or_min == "max"){
    100 * (length(which(dist_data < log((100 - percent)/100)))) / num_points
  } else {
    100 * (length(which(dist_data > log((100 + percent)/100)))) / num_points
  }
}
risk_percentiles <- Vectorize(risk_percentiles, "percent")
  
## Estimate risk
risk_estimate <- function(dist_data, max_or_min = "max", percent) {
  if(max_or_min == "max") {
    cat("What is the risk of losing max", percent,"%? =<", risk_percentiles(dist_data, percent, max_or_min), "percent\n")
  } else {
    cat("What is the chance of gaining min", percent,"%? >=",risk_percentiles(dist_data, percent, max_or_min), "percent\n")
  }
}

## down_and_out_df() takes a data frame as input.
## For each column, the first element which is below the threshold and all following elements
## are set to 0.
down_and_out_df <- function(df, threshold) {
  ll <- lapply(df,
    function(vect) {
      n <- length(vect) 
      for(i in seq_along(vect)) {
        if(vect[i] < threshold ) {
          if(i == n) {break}
          vect[(i):n] <- rep(0, n - i + 1)
          break
        }
      }
      vect
    }
  )
  as.data.frame(ll)
}

down_and_out_vect <- function(vect, threshold) {
  n <- length(vect) 
  for(i in seq_along(vect)) {
    if(vect[i] < threshold ) {
      if(i == n) {break}
      vect[(i):n] <- rep(0, n - i + 1)
      break
    }
  }
  vect
}

count_num_dao <- function(df, threshold = 0) {
  length(which(df[nrow(df), ] <= threshold))
}

df_summary_to_df <- function(df_summary) {
  summary_df_pre <- as.data.frame(df_summary)
  summary_levels <- levels(summary_df_pre$Var2)
  summary_df <- data.frame(
    matrix(
      length(summary_levels), 
      nrow=nrow(
        summary_df_pre[summary_df_pre$Var2 == summary_levels[[1]], ]
      )
    )
  )
  summary_df_2 <- summary_df
  for(i in seq_along(summary_levels)) {
    summary_df[, i] <- summary_df_pre[summary_df_pre$Var2 == summary_levels[i], "Freq"]
  }

  row_names <- c("Min.   :", "1st Qu.:", "Median :", "Mean   :", "3rd Qu.:", "Max.   :")
  for(i in 1:nrow(summary_df)) {
    for(j in 1:ncol(summary_df)) {
      summary_df_2[i, j] <- as.numeric(gsub(row_names[i], "", summary_df[i, j]))
    }
  }
  names(summary_df_2) <- unlist(lapply(summary_levels, function(x) {gsub(" ", "", x)}))
  row.names(summary_df_2) <- row_names
  summary_df_2
}


## Fit data to a skewed t distribution
fit_skewed_t <- function(x, method = "BFGS") {
  loglik_sstd = function(beta) sum(- dsstd(x, mean = beta[1], sd = beta[2], nu = beta[3], xi = beta[4], log = TRUE))
  start = c(mean(x), sd(x), 3, 1)
  #fit_sstd = optim(start, loglik_sstd, hessian = F, method="L-BFGS-B", lower = c(0, 0.1, 1.1, -2))
  fit_sstd = optim(start, loglik_sstd, method = method)

  n = length(x)
  
  AIC_sstd = 2 * fit_sstd$value + 2 * 4
  BIC_sstd = 2 * fit_sstd$value + log(n) * 4
  #sd_sstd = sqrt(diag(solve(fit_sstd$hessian)))
  cat("\n")
  #cat("sd_sstd:", sd_sstd)
  cat("AIC:", AIC_sstd, "\n")
  cat("BIC:", BIC_sstd, "\n")
  
  # MiddelvÃ¦rdi:
  mu_sstd_fit <- fit_sstd$par[1]#/1000
  cat("m:", mu_sstd_fit, "\n")
  
  # Spredning:
  sigma_sstd_fit <- fit_sstd$par[2]#/1000
  cat("s:", sigma_sstd_fit, "\n")
  
  # Frihedsgrader:
  nu_sstd_fit <- fit_sstd$par[3] # 3.36019
  cat("nu (df):", nu_sstd_fit, "\n")
  
  xi_sstd_fit <- fit_sstd$par[4] # 0.8436514
  cat("xi:", xi_sstd_fit, "\n")
  
  fit <- qsstd((1:n - 0.5)/n, mean = fit_sstd$par[1], sd = fit_sstd$par[2], nu = fit_sstd$par[3], xi = fit_sstd$par[4])

  r_squared <- cor(sort(fit), sort(x))
  # R^2
  r_squared_round <- round(r_squared, 3)
  cat("R^2:", r_squared_round, "\n")
  cat("\n")
  
  range_cuts <- c(0.0, 0.5, 0.9, 0.95, 0.99, 1.0)
  interpretations <- c(
    paste0("An R^2 of ", r_squared_round, " suggests that the fit is not great."),
    paste0("An R^2 of ", r_squared_round, " suggests that the fit is not completely random."),
    paste0("An R^2 of ", r_squared_round, " suggests that the fit is good."),
    paste0("An R^2 of ", r_squared_round, " suggests that the fit is very good."),
    paste0("An R^2 of ", r_squared_round, " suggests that the fit is extremely good.")
  )
  
  cat(interpretations[findInterval(r_squared_round, range_cuts)])
  cat("\n")
  cat("\n")
  
  dist_data <- qsstd((1:1000 - 0.5)/1000, mean = fit_sstd$par[1], sd = fit_sstd$par[2], nu = fit_sstd$par[3], xi = fit_sstd$par[4])

  risk_estimate(dist_data, "max", 10)
  risk_estimate(dist_data, "max", 25)
  risk_estimate(dist_data, "max", 50)
  risk_estimate(dist_data, "max", 90)
  risk_estimate(dist_data, "max", 99)
  cat("\n")
  risk_estimate(dist_data, "min", 10)
  risk_estimate(dist_data, "min", 25)
  risk_estimate(dist_data, "min", 50)
  risk_estimate(dist_data, "min", 90)
  risk_estimate(dist_data, "min", 99)

  theoretical_quantiles <- qsstd(ppoints(n), mu_sstd_fit, sigma_sstd_fit, nu_sstd_fit, xi_sstd_fit)
  
  output <- list(
    # Plot with estimated skewed t-distribution
    # qq plot for t-distribution
    qqplot = function() {qqplot(x = x, y = theoretical_quantiles, main = "QQ-plot, skewed t", xlab = "log returns", ylab = "skewed t-quantiles",  xlim = c(min(x) - abs(min(x))/10, max(x) + abs(max(x))/10), ylim = c(min(fit) - abs(min(fit))/10, max(fit) + abs(max(fit))/10))
    mtext(paste0("my=", round(mu_sstd_fit, 4),", sigma =", round(sigma_sstd_fit, 4),", df=",round(nu_sstd_fit, 3), ", xi =", round(xi_sstd_fit, 3), ", R^2=", r_squared_round), side=3,  cex = 0.8, adj=0)
    qqline(x, distribution = function(p) qsstd(p, mu_sstd_fit, sigma_sstd_fit, nu_sstd_fit, xi_sstd_fit), datax = TRUE, col="black")
    abline(0, 1, col = "red")
    legend("bottomright", legend = c("data trendline", "45 degree line"), col = c("black", "red"), lty = c(1, 1))
    },
    fit_plot = function() {
      plot(sort(x), col="blue", ylab = "log returns", main = "Data vs fit",
           ylim = c(min(c(x, fit)), max(c(x, fit))))
      points(sort(fit), col="red")
      legend("bottomright", legend = c("data", "fit"), col = c("blue", "red"), pch = c(1, 1))
    },
    dist_plot = function() {
      plot(sort(dist_data), pch = 16, cex = 0.3, main = "Estimated skew t distribution CDF")
      abline(a = 0, b = 0, col = "red")
    },
    data = x,
    fit = fit,
    dist_data = dist_data,
    m = mu_sstd_fit,
    s = sigma_sstd_fit,
    nu = nu_sstd_fit,
    xi = xi_sstd_fit,
    r_squared = r_squared,
    theoretical_quantiles = theoretical_quantiles
  )
  
  comment(output) <- "single_fit"
  output
}


# mc_simulation <- function(fit, num_paths = 1000, num_periods = 20, dao = TRUE) {
#   init_capital = 100
# 
#   qual_col_pals = brewer.pal.info[brewer.pal.info$category == 'qual',]
#   col_vector = unlist(mapply(brewer.pal, qual_col_pals$maxcolors, rownames(qual_col_pals)))
# 
#   mc_df <- data.frame(rep(0, num_periods))
#   for(j in 1:num_paths) {
#     mc_df[ ,j] <- init_capital * exp(cumsum(rsstd(num_periods, fit$m, fit$s, fit$nu, fit$xi)))
#   }
# 
#   colnames(mc_df) <- 1:num_paths
#   
#   mc_m <- mean(unlist(mc_df[num_periods, ]))
#   mc_s <- sd(unlist(mc_df[num_periods, ]))
#   mc_min <- min(unlist(mc_df[num_periods, ]))
#   mc_max <- max(unlist(mc_df[num_periods, ]))
#   
#   cat("Mean total portfolio index value after", num_periods, "years:", mc_m, "kr.\n")
#   cat("SD of total portfolio index value after", num_periods, "years:", mc_s, "kr.\n")
#   cat("Min total portfolio index value after", num_periods, "years:", mc_min, "kr.\n")
#   cat("Max total portfolio index value after", num_periods, "years:", mc_max, "kr.\n")
#   cat("\n")
#   
#   list(
#     mc_plot = function() {
#       plot(mc_df[, 1], type = "l", ylim = c(min(mc_df), max(mc_df)), xlab = "period", ylab = "Portfolio index value in kr", )
#       lapply(mc_df[, -1], function(x) {
#         lines(x, col = alpha(sample(col_vector, num_paths, replace = TRUE), 0.3))}
#       )
#       #polygon(x = c(0, num_periods, num_periods, 0), y = c(min(mc_df), min(mc_df), 0, 0), col = rgb(0.4, 0.4, 0.4, 0.25), border = NA)
#       abline(100, 0, lwd=1, col="red")
#     },
#     mc_plot_last_period = function() {
#       plot(
#         sort(unlist(mc_df[num_periods, ])), pch = 16, cex = 0.3,
#         xlab = "Sample ID",
#         ylab = "Portfolio index value in kr"
#       )
#       #polygon(x = c(0, num_paths, num_paths, 0), y = c(min(mc_df), min(mc_df), 0, 0), col = rgb(0.4, 0.4, 0.4, 0.25), border = NA)
#       abline(100, 0, lwd=1, col="red")
#       mtext(side=3, line=2, at=-0.07, adj=0, cex=1, "Sorted portfolio index values for last period of all runs")
#       mtext(side=3, line=1, at=-0.07, adj=0, cex=0.7, "(100 is par, 200 is double, 50 is half)")
#     },
#     mc_m = mc_m,
#     mc_s = mc_s,
#     mc_min = mc_min,
#     mc_max = mc_max,
#     mc_df = mc_df
#   )
# }


## Monte Carlo simulation
## Creates a data frame where each column is a simulated series, and the number 
## of rows is the number of periods in each series.
## The fit parameter can be a single output from fit_skewed_t(), or a list of 
## outputs from fit_skewed_t().
## mc_simulation() will create a separate simulation for each fit, dividing an 
## initial value of 100 between them, and then add the simulations together.
## For instance, if there are two fits, the first column of the simulations data 
## frame for the first fit will be added to the first column of the second df, ect.
## If dao=TRUE, will calculate down-and-out paths. For each column, the first 
## element which is below the threshold and all following elements are set to 0.

mc_simulation <- function(
    fit, 
    num_paths = 1000, 
    num_periods = 20,
    dao = TRUE,
    threshold = 0.01
    ) {
  init_capital = 100

  qual_col_pals = brewer.pal.info[brewer.pal.info$category == 'qual',]
  col_vector = unlist(mapply(brewer.pal, qual_col_pals$maxcolors, rownames(qual_col_pals)))

  ## Each single fit has comment "single_fit".
  ## A list if fits will not have a comment.
  if(is.null(comment(fit))) {
    ## A list of fits
    init_capital = 100/length(fit)
    mc_df <- data.frame(matrix(rep(0, num_paths * num_periods), nrow = num_periods))
    for(i in seq_along(fit)) {
      for(j in 1:num_paths) {
        mc_df[ ,j] <- mc_df[ ,j] + c(init_capital, init_capital * exp(cumsum(rsstd(num_periods - 1, fit[[i]]$m, fit[[i]]$s, fit[[i]]$nu, fit[[i]]$xi))))
      }
    }
  } else {
    if(comment(fit) == "single_fit") {
      mc_df <- data.frame(rep(0, num_periods))
      for(j in 1:num_paths) {
        mc_df[ ,j] <- c(init_capital, init_capital * exp(cumsum(rsstd(num_periods - 1, fit$m, fit$s, fit$nu, fit$xi))))
      }
    }
  }
  

  if(dao == TRUE) {
    mc_df <- down_and_out_df(mc_df, threshold)
    num_dao <- count_num_dao(mc_df)
    dao_probability_percent <- 100 * num_dao/num_paths
    
    cat("Down-and-out simulation:\n")
    cat("Probability of down-and-out:", dao_probability_percent, "percent\n")
    cat("\n")
  } else {
    cat("Simulation (ignoring down-and-out):\n")
    dao_probability_percent <- NA
  }

  colnames(mc_df) <- 1:num_paths

  mc_m <- mean(unlist(mc_df[num_periods, ]))
  mc_s <- sd(unlist(mc_df[num_periods, ]))
  mc_min <- min(unlist(mc_df[num_periods, ]))
  mc_max <- max(unlist(mc_df[num_periods, ]))
  
  percent_losing_paths <- 100 * count_num_dao(mc_df, threshold = 100)/num_paths

  cat("Mean portfolio index value after", num_periods, "years:", round(mc_m, 3), "kr.\n")
  cat("SD of portfolio index value after", num_periods, "years:", round(mc_s, 3), "kr.\n")
  cat("Min total portfolio index value after", num_periods, "years:", round(mc_min, 3), "kr.\n")
  cat("Max total portfolio index value after", num_periods, "years:", round(mc_max, 3), "kr.\n")
  cat("\n")
  cat("Share of paths finishing below 100:", percent_losing_paths, "percent")

  list(
    mc_plot = function() {
      plot(mc_df[, 1], type = "l", ylim = c(min(mc_df), max(mc_df)), xlab = "period", ylab = "Portfolio index value in kr", main = paste0("MC simulation ", ifelse(dao == TRUE, "with down-and-out", "ignoring down-and-out")), sub = paste0("Number of paths: ", num_paths, ", number of periods: ", num_periods))
      lapply(mc_df[, -1], function(x) {
        lines(x, col = alpha(sample(col_vector, num_paths, replace = TRUE), 0.3))}
      )
      abline(100, 0, lwd=1, col="red")
    },
    mc_plot_last_period = function() {
      plot(
        sort(unlist(mc_df[num_periods, ])), pch = 16, cex = 0.3,
        xlab = "Sample ID",
        ylab = "Portfolio index value in kr"
      )
      abline(100, 0, lwd=1, col="red")
      mtext(side=3, line=2, at=-0.07, adj=0, cex=1, "Sorted portfolio index values for last period of all runs")
      mtext(side=3, line=1, at=-0.07, adj=0, cex=0.7, "(100 is par, 200 is double, 50 is half)")
    },
    mc_m = mc_m,
    mc_s = mc_s,
    mc_min = mc_min,
    mc_max = mc_max,
    mc_df = mc_df,
    dao_probability_percent = dao_probability_percent,
    percent_losing_paths = percent_losing_paths
  )
}
```




# Log returns data 2011-2023.  
For 2011, medium risk data is used in the high risk data set, as no high risk fund data is available prior to 2012.  
`vmrl` is a long version of Velliv medium risk data, from 2007 to 2023. For 2007 to 2011 (both included) no high risk data is available.
```{r}
# Velliv medium risk returns
vmr <- log(c(0.986, 1.098, 1.157, 1.136, 1.058, 1.044, 1.100, 0.963, 1.168, 1.097, 1.141, 0.868, 1.094))

# Velliv medium risk return, long period: 2007-2023
vmrl <- log(c(1.058, 0.801, 1.193, 1.129, 1.035, 0.996, 1.133, 1.092, 1.085, 1.013, 1.095, 1.011, 1.129, 1.101, 1.128, 0.988, 1.048))

# PFA medium risk returns
pmr <- log(c(1.084, 0.904, 1.107, 1.042, 1.119, 0.976, 1.089, 1.073, 1.084, 1.107, 1.111, 1.141, 1.004))

# Mix medium risk return
mmr <- log(c(1.035, 0.996, 1.133, 1.092, 1.085, 1.013, 1.095, 1.011, 1.129, 1.101, 1.128, 0.988, 1.048))

# Velliv high risk returns
vhr <- log(c(0.986, 1.093, 1.214, 1.160, 1.075, 1.039, 1.118, 0.952, 1.205, 1.099, 1.194, 0.849, 1.122))

# PFA high risk return
phr <- log(c(1.159, 0.878, 1.200, 1.068, 1.190, 0.943, 1.135, 1.082, 1.123, 1.128, 1.208, 1.182, 0.934))
  
# Mix high risk return
mhr <- log(c(1.073, 0.977, 1.207, 1.116, 1.128, 0.992, 1.126, 1.013, 1.164, 1.113, 1.201, 1.012, 1.014))

data_df <- data.frame(
  vmr = vmr,
  pmr = pmr,
  mmr = mmr,
  vhr = vhr,
  phr = phr,
  mhr = mhr
)

data_df_l <- data.frame(
  vmrl = vmrl
)
```

```{r}
all_data <- exp(c(vmr, vhr, pmr, phr, mmr, mhr))
plot(x = c(2011:2023), y = exp(vmr), type = "l",  lty = 1, lwd = 2, xlab = "year", ylab = "Gross returns", col = "#7AC5CD", ylim = c(min(all_data), max(all_data)), main = "Gross returns 2011-2023")
lines(x = c(2011:2023), y = exp(vhr), col = "#76EEC6",  lty = 2, lwd = 1.5)
lines(x = c(2011:2023), y = exp(pmr), col = "#CD6600",  lty = 1, lwd = 2)
lines(x = c(2011:2023), y = exp(phr), col = "#CDAD00",  lty = 2, lwd = 1.5)
lines(x = c(2011:2023), y = exp(mmr), col = "#A020F0",  lty = 1, lwd = 2)
lines(x = c(2011:2023), y = exp(mhr), col = "#FF00FF",  lty = 2, lwd = 1.5)
legend("bottom", legend = c("vmr", "vhr", "pmr", "phr", "mmr", "mhr"), col = c("#7AC5CD", "#76EEC6", "#CD6600", "#CDAD00", "#A020F0", "#FF00FF"), lty = c(1, 2, 1, 2, 1, 2), lwd = 2, ncol = 3 )
```

Summary of gross returns
```{r}
df_summary <- summary(exp(data_df))
df_summary
```

```{r}
summary(exp(data_df_l))
```
```{r}
summary_df <- df_summary_to_df(df_summary)

cat("Highest minimum log-return:", colnames(summary_df)[which(summary_df[1, 1:6] == max(summary_df[1, 1:6]))], "\n")

cat("Highest median log-return:", colnames(summary_df)[which(summary_df[3, 1:6] == max(summary_df[3, 1:6]))], "\n")

cat("Highest mean log-return:", colnames(summary_df)[which(summary_df[4, 1:6] == max(summary_df[4, 1:6]))], "\n")

cat("Highest max log-return:", colnames(summary_df)[which(summary_df[6, 1:6] == max(summary_df[6, 1:6]))], "\n")
```



```{r}
cat("cov(vmr, pmr) = ", cov(vmr, pmr), "\n")
cat("cov(vhr, phr) = ", cov(vhr, phr), "\n")
```



# Velliv medium risk, 2011 - 2023
```{r}
fit_vmr <- fit_skewed_t(vmr)
```

## QQ Plot
```{r}
(fit_vmr$qqplot)()
```

The qq plot looks great. Log returns for Velliv medium risk seems to be consistent with a skewed t-distribution.  


## Data vs fit
Let's plot the fit and the observed returns together.  

```{r}
(fit_vmr$fit_plot)()
```

## Estimated distribution
Now lets look at the CDF of the estimated distribution for each 0.1% increment between 0.5% and 99.5% for the estimated distribution:

```{r}
(fit_vmr$dist_plot)()
```

We see that for a few observations out of a 1000, the losses are disastrous, while the upside is very dampened.


## Monte Carlo

```{r}
mc_vmr <- mc_simulation(fit_vmr, num_paths = mc_num_paths, num_periods = mc_num_periods, dao = mc_dao)
```

```{r results='hide',fig.keep='all'}
(mc_vmr$mc_plot)()
```


```{r results='hide',fig.keep='all'}
(mc_vmr$mc_plot_last_period)()
```

# Velliv medium risk, 2007 - 2023

## Fit to skew t distribution
```{r}
fit_vmrl <- fit_skewed_t(vmrl)
```


## QQ Plot

```{r}
(fit_vmrl$qqplot)()
```

The qq plot looks good. Log returns for Velliv high risk seems to be consistent with a skewed t-distribution.  


## Data vs fit
Let's plot the fit and the observed returns together.  

```{r}
(fit_vmrl$fit_plot)()
```


## Estimated distribution
Now lets look at the CDF of the estimated distribution for each 0.1% increment between 0.5% and 99.5% for the estimated distribution:

```{r}
(fit_vmrl$dist_plot)()
```

We see that for a few observations out of a 1000, the losses are disastrous, while the upside is very dampened. But because the disastrous loss in 2008 was followed by a large profit the following year, we see some increased upside for the top percentiles. Beware: A 1.2 return following a 0.8 return doesn't take us back where we were before the loss. Path dependency! So if returns more or less average out, but high returns have a tendency to follow high losses, that's bad!


## Monte Carlo

```{r}
mc_vmrl <- mc_simulation(fit_vmrl, num_paths = mc_num_paths, num_periods = mc_num_periods, dao = mc_dao)
```

```{r results='hide',fig.keep='all'}
(mc_vmrl$mc_plot)()
```



```{r results='hide',fig.keep='all'}
(mc_vmrl$mc_plot_last_period)()
```


# Velliv high risk, 2011 - 2023

## Fit to skew t distribution
```{r}
fit_vhr <- fit_skewed_t(vhr, method = "Nelder-Mead")
```


## QQ Plot
```{r}
(fit_vhr$qqplot)()
```

The qq plot looks great. Returns for Velliv medium risk seems to be consistent with a skewed t-distribution.  

## Data vs fit
Let's plot the fit and the observed returns together.  

```{r}
(fit_vhr$fit_plot)()
```

## Estimated distribution
Now lets look at the CDF of the estimated distribution for each 0.1% increment between 0.5% and 99.5% for the estimated distribution:

```{r}
(fit_vhr$dist_plot)()
```

We see that for a few observations out of a 1000, the losses are disastrous, while the upside is very dampened.


## Monte Carlo

```{r}
mc_vhr <- mc_simulation(fit_vhr, num_paths = mc_num_paths, num_periods = mc_num_periods, dao = mc_dao)
```

```{r results='hide',fig.keep='all'}
(mc_vhr$mc_plot)()
```



```{r results='hide',fig.keep='all'}
(mc_vhr$mc_plot_last_period)()
```


# PFA medium risk, 2011 - 2023

## Fit to skew t distribution
```{r}
fit_pmr <- fit_skewed_t(pmr)
```


## QQ Plot
```{r}
(fit_pmr$qqplot)()
```

The qq plot looks great. Log returns for PFA medium risk seems to be consistent with a skewed t-distribution.  


```{r}
fit_pmr$theoretical_quantiles
```


## Data vs fit
Let's plot the fit and the observed returns together.  

```{r}
(fit_pmr$fit_plot)()
```

## Estimated distribution
Now lets look at the CDF of the estimated distribution for each 0.1% increment between 0.5% and 99.5% for the estimated distribution:

```{r}
(fit_pmr$dist_plot)()
```

We see that for a few observations out of a 1000, the losses are disastrous. While there is some uptick at the top percentiles, the curve basically flattens out.


## Monte Carlo

```{r}
mc_pmr <- mc_simulation(fit_pmr, num_paths = mc_num_paths, num_periods = mc_num_periods, dao = mc_dao)
```

```{r results='hide',fig.keep='all'}
(mc_pmr$mc_plot)()
```



```{r results='hide',fig.keep='all'}
(mc_pmr$mc_plot_last_period)()
```


# PFA high risk, 2011 - 2023

## Fit to skew t distribution
```{r}
fit_phr <- fit_skewed_t(phr)
```


## QQ Plot
```{r}
(fit_phr$qqplot)()
```

The qq plot looks ok. Returns for PFA high risk seems to be consistent with a skewed t-distribution.  

## Data vs fit
Let's plot the fit and the observed returns together.  

```{r}
(fit_phr$fit_plot)()
```

## Estimated distribution
Now lets look at the CDF of the estimated distribution for each 0.1% increment between 0.5% and 99.5% for the estimated distribution:

```{r}
(fit_phr$dist_plot)()
```

We see that for a few observations out of a 1000, the losses are disastrous, while the upside is very dampened.


## Monte Carlo

```{r}
mc_phr <- mc_simulation(fit_phr, num_paths = mc_num_paths, num_periods = mc_num_periods, dao = mc_dao)
```

```{r results='hide',fig.keep='all'}
(mc_phr$mc_plot)()
```



```{r results='hide',fig.keep='all'}
(mc_phr$mc_plot_last_period)()
```



# Mix medium risk, 2011 - 2023

## Fit to skew t distribution
```{r}
fit_mmr <- fit_skewed_t(mmr)
```


## QQ Plot
```{r}
(fit_mmr$qqplot)()
```

The fit suggests big losses for the lowest percentiles, which are not present in the data.  
So the fit is actually a very cautious estimate.


## Data vs fit
Let's plot the fit and the observed returns together.  

```{r}
(fit_mmr$fit_plot)()
```

Interestingly, the fit predicts a much bigger "biggest loss" than the actual data. This is the main reason that R^2 is 0.90 and not higher.


## Estimated distribution
Now lets look at the CDF of the estimated distribution for each 0.1% increment between 0.5% and 99.5% for the estimated distribution:

```{r}
(fit_mmr$dist_plot)()
```

We see that for a few observations out of a 1000, the losses are disastrous, while the upside is very dampened.


## Monte Carlo

### Version a: Simulation from estimated distribution of returns of mix.  
```{r}
mc_mmr_a <- mc_simulation(fit_mmr, num_paths = mc_num_paths, num_periods = mc_num_periods, dao = mc_dao)
```

```{r results='hide',fig.keep='all'}
(mc_mmr_a$mc_plot)()
```



```{r results='hide',fig.keep='all'}
(mc_mmr_a$mc_plot_last_period)()
```


```{r include=FALSE}
# NOTE: Can we just use the mixed returns, when we are mixing two processes with individual path dependence? (See appendix)
# 
# Instead we try doing one MC simulation for `vmr` and one for `pmr`, each with initial value 50, then adding the two simulations to each other.
```

### Version b: Mix of simulations from estimated distribution of returns from individual funds.


```{r}
mc_mmr_b <- mc_simulation(list(fit_vmr, fit_pmr), num_paths = mc_num_paths, num_periods = mc_num_periods, dao = mc_dao)
```

```{r results='hide',fig.keep='all'}
(mc_mmr_b$mc_plot)()
```


```{r results='hide',fig.keep='all'}
(mc_mmr_b$mc_plot_last_period)()
```


# Mix high risk, 2011 - 2023

## Fit to skew t distribution
```{r}
fit_mhr <- fit_skewed_t(mhr)
```


## QQ Plot
```{r}
(fit_mhr$qqplot)()
```

The qq plot looks good Returns for mixed medium risk portfolios seems to be consistent with a skewed t-distribution.  


## Data vs fit
Let's plot the fit and the observed returns together.  

```{r}
(fit_mhr$fit_plot)()
```

## Estimated distribution
Now lets look at the CDF of the estimated distribution for each 0.1% increment between 0.5% and 99.5% for the estimated distribution:

```{r}
(fit_mhr$dist_plot)()
```

We see that the high risk mix provides a much better upside and smaller downside.

## Monte Carlo

### Version a: Simulation from estimated distribution of returns of mix.  
```{r}
mc_mhr_a <- mc_simulation(fit_mhr, num_paths = mc_num_paths, num_periods = mc_num_periods, dao = mc_dao)
```

```{r results='hide',fig.keep='all'}
(mc_mhr_a$mc_plot)()
```



```{r results='hide',fig.keep='all'}
(mc_mhr_a$mc_plot_last_period)()
```

```{r}
# NOTE: Can we just use the mixed returns, when we are mixing two processes with individual path dependence? (See appendix)
# 
# Instead we try doing one MC simulation for `vhr` and one for `phr`, each with initial value 50, then adding the two simulations to each other.
```


### Version b: Mix of simulations from estimated distribution of returns from individual funds.

```{r}
mc_mhr_b <- mc_simulation(list(fit_vhr, fit_phr), num_paths = mc_num_paths, num_periods = mc_num_periods, dao = mc_dao)
```

```{r results='hide',fig.keep='all'}
(mc_mhr_b$mc_plot)()
```


```{r results='hide',fig.keep='all'}
(mc_mhr_b$mc_plot_last_period)()
```






# Compare pension plans

Risk of max loss of x percent for a single period (year).  
x values are row names.  

```{r}
# Use "max" for losses
percent_vals = c(0, 5, 10, 25, 50, 90, 99)
risk_percentiles_df <-  data.frame(
      #Percent = percent_vals,
      Velliv_medium = risk_percentiles(fit_vmr$dist_data, percent_vals, "max"),
      Velliv_medium_long = risk_percentiles(fit_vmrl$dist_data, percent_vals, "max"),
      Velliv_high = risk_percentiles(fit_vhr$dist_data, percent_vals, "max"),
      PFA_medium = risk_percentiles(fit_pmr$dist_data, percent_vals, "max"),
      PFA_high = risk_percentiles(fit_phr$dist_data, percent_vals, "max"),
      mix_medium = risk_percentiles(fit_mmr$dist_data, percent_vals, "max"),
      mix_high = risk_percentiles(fit_mhr$dist_data, percent_vals, "max")
)

rownames(risk_percentiles_df) <- as.character(percent_vals)
```

```{r}
knitr::kable(risk_percentiles_df, digits = 3)
```




Chance of min gains of x percent for a single period (year).  
x values are row names.

```{r}
# Use "min" for gains
percent_vals = c(0, 5, 10, 25, 50, 100)
gain_percentiles_df <-  data.frame(
      #Percent = percent_vals,
      Velliv_medium = risk_percentiles(fit_vmr$dist_data, percent_vals, "min"),
      Velliv_medium_long = risk_percentiles(fit_vmrl$dist_data, percent_vals, "min"),
      Velliv_high = risk_percentiles(fit_vhr$dist_data, percent_vals, "min"),
      PFA_medium = risk_percentiles(fit_pmr$dist_data, percent_vals, "min"),
      PFA_high = risk_percentiles(fit_phr$dist_data, percent_vals, "min"),
      mix_medium = risk_percentiles(fit_mmr$dist_data, percent_vals, "min"),
      mix_high = risk_percentiles(fit_mhr$dist_data, percent_vals, "min")
)

rownames(gain_percentiles_df) <- as.character(percent_vals)
```


```{r}
knitr::kable(gain_percentiles_df, digits = 3)
```

MC risk percentiles: Risk of loss from first to last period.  
`_a` is simulation from estimated distribution of returns of mix.  
`_b` is mix of simulations from estimated distribution of returns from individual 
funds.

`_m` is medium.  
`_h`  is high.  

```{r}
# Use "max" for losses
percent_vals = c(0, 5, 10, 25, 50, 90, 99)
mc_loss_percentiles_df <-  data.frame(
      #Percent = percent_vals,
      Velliv_m = risk_percentiles(log(unlist(mc_vmr$mc_df[20, ])/100), percent_vals, "max"),
      Velliv_m_long = risk_percentiles(log(unlist(mc_vmrl$mc_df[20, ])/100), percent_vals, "max"),
      Velliv_h = risk_percentiles(log(unlist(mc_vhr$mc_df[20, ])/100), percent_vals, "max"),
      PFA_m = risk_percentiles(log(unlist(mc_pmr$mc_df[20, ])/100), percent_vals, "max"),
      PFA_h = risk_percentiles(log(unlist(mc_phr$mc_df[20, ])/100), percent_vals, "max"),
      mix_m_a = risk_percentiles(log(unlist(mc_mmr_a$mc_df[20, ])/100), percent_vals, "max"),
      mix_h_a = risk_percentiles(log(unlist(mc_mhr_a$mc_df[20, ])/100), percent_vals, "max"),
      mix_m_b = risk_percentiles(log(unlist(mc_mmr_b$mc_df[20, ])/100), percent_vals, "max"),
      mix_h_b = risk_percentiles(log(unlist(mc_mhr_b$mc_df[20, ])/100), percent_vals, "max")
)

rownames(mc_loss_percentiles_df) <- as.character(percent_vals)
```

```{r}
knitr::kable(mc_loss_percentiles_df, digits = 3)
```


MC gains percentiles: Chance of gains from first to last period.  
`_a` is simulation from estimated distribution of returns of mix.  
`_b` is mix of simulations from estimated distribution of returns from individual 
funds.

```{r}
# Use "min" for gains
percent_vals = c(0, 5, 10, 25, 50, 100, 200, 300, 400, 500, 1000)
mc_gain_percentiles_df <-  data.frame(
      #Percent = percent_vals,
      Velliv_m = risk_percentiles(log(unlist(mc_vmr$mc_df[20, ])/100), percent_vals, "min"),
      Velliv_m_long = risk_percentiles(log(unlist(mc_vmrl$mc_df[20, ])/100), percent_vals, "min"),
      Velliv_h = risk_percentiles(log(unlist(mc_vhr$mc_df[20, ])/100), percent_vals, "min"),
      PFA_m = risk_percentiles(log(unlist(mc_pmr$mc_df[20, ])/100), percent_vals, "min"),
      PFA_h = risk_percentiles(log(unlist(mc_phr$mc_df[20, ])/100), percent_vals, "min"),
      mix_m_a = risk_percentiles(log(unlist(mc_mmr_a$mc_df[20, ])/100), percent_vals, "min"),
      mix_h_a = risk_percentiles(log(unlist(mc_mhr_a$mc_df[20, ])/100), percent_vals, "min"),
      mix_m_b = risk_percentiles(log(unlist(mc_mmr_b$mc_df[20, ])/100), percent_vals, "min"),
      mix_h_b = risk_percentiles(log(unlist(mc_mhr_b$mc_df[20, ])/100), percent_vals, "min")
)

rownames(mc_gain_percentiles_df) <- as.character(percent_vals)
```

```{r}
knitr::kable(mc_gain_percentiles_df, digits = 3)
```


## Summary statistics  

### Fit summary
Summary for fit of log returns to an F-S skew standardized Student-t distribution.  
`m`  is the location parameter.  
`s` is the scale parameter.  
`nu` is the estimated degrees of freedom, or shape parameter.  
`xi` is the estimated skewness parameter.  

```{r}
fit_summary <- data.frame(
  Velliv_medium = c(fit_vmr$m, fit_vmr$s, fit_vmr$nu, fit_vmr$xi, fit_vmr$r_squared),
  Velliv_medium_long = c(fit_vmrl$m, fit_vmrl$s, fit_vmrl$nu, fit_vmrl$xi, fit_vmrl$r_squared),
  Velliv_high = c(fit_vhr$m, fit_vhr$s, fit_vhr$nu, fit_vhr$xi, fit_vhr$r_squared),
  PFA_medium = c(fit_pmr$m, fit_pmr$s, fit_pmr$nu, fit_pmr$xi, fit_pmr$r_squared),
  PFA_high = c(fit_phr$m, fit_phr$s, fit_phr$nu, fit_phr$xi, fit_phr$r_squared),
  mix_medium = c(fit_mmr$m, fit_mmr$s, fit_mmr$nu, fit_mmr$xi, fit_mmr$r_squared),
  mix_high = c(fit_mhr$m, fit_mhr$s, fit_mhr$nu, fit_mhr$xi, fit_mhr$r_squared)
)

rownames(fit_summary) <- c("m", "s", "nu", "xi", "R-squared")
```

```{r}
knitr::kable(fit_summary, digits = 3)
```


### Monte Carlo simulations summary

Monte Carlo simulations of portfolio index values (currency values).  
Statistics are given for the final state of all paths.  
Probability of down-and_out is calculated as the share of paths that reach 0 at
some point. All subsequent values for a path are set to 0, if the path reaches
at any point.  
0 is defined as any value below a threshold.  
`losing_prob_pct` is the probability of losing money. This is calculated as the 
share of paths finishing below index 100.  

```{r}
cat("Number of paths:", mc_num_paths, "\n")
```


```{r}
mc_summary <- data.frame(
  Velliv_m = c(mc_vmr$mc_m, mc_vmr$mc_s, mc_vmr$mc_min, mc_vmr$mc_max, mc_vmr$dao_probability_percent, mc_vmr$percent_losing_paths),
  Velliv_m_long = c(mc_vmrl$mc_m, mc_vmrl$mc_s, mc_vmrl$mc_min, mc_vmrl$mc_max, mc_vmrl$dao_probability_percent, mc_vmrl$percent_losing_paths),
  Velliv_h = c(mc_vhr$mc_m, mc_vhr$mc_s, mc_vhr$mc_min, mc_vhr$mc_max, mc_vhr$dao_probability_percent, mc_vhr$percent_losing_paths),
  PFA_m = c(mc_pmr$mc_m, mc_pmr$mc_s, mc_pmr$mc_min, mc_pmr$mc_max, mc_pmr$dao_probability_percent, mc_pmr$percent_losing_paths),
  PFA_h = c(mc_phr$mc_m, mc_phr$mc_s, mc_phr$mc_min, mc_phr$mc_max, mc_phr$dao_probability_percent, mc_phr$percent_losing_paths),
  mix_m_a = c(mc_mmr_a$mc_m, mc_mmr_a$mc_s, mc_mmr_a$mc_min, mc_mmr_a$mc_max, mc_mmr_a$dao_probability_percent, mc_mmr_a$percent_losing_paths),
  mix_m_b = c(mc_mmr_b$mc_m, mc_mmr_b$mc_s, mc_mmr_b$mc_min, mc_mmr_b$mc_max, mc_mmr_b$dao_probability_percent, mc_mmr_b$percent_losing_paths),
  mix_h_a = c(mc_mhr_a$mc_m, mc_mhr_a$mc_s, mc_mhr_a$mc_min, mc_mhr_a$mc_max, mc_mhr_a$dao_probability_percent, mc_mhr_a$percent_losing_paths),
  mix_h_b = c(mc_mhr_b$mc_m, mc_mhr_b$mc_s, mc_mhr_b$mc_min, mc_mhr_b$mc_max, mc_mhr_b$dao_probability_percent, mc_mhr_b$percent_losing_paths)
)

rownames(mc_summary) <- c("mc_m", "mc_s", "mc_min", "mc_max", "dao_prob_pct", "losing_prob_pct")
```

```{r}
knitr::kable(mc_summary, digits = 3)
```

```{r}
cat("Highest mean    :", colnames(mc_summary)[which(mc_summary[1, ] == max(mc_summary[1, ]))], "\n")

cat("Lowest sd       :", colnames(mc_summary)[which(mc_summary[2, ] == min(mc_summary[2, ]))], "\n")

cat("Highest min     :", colnames(mc_summary)[which(mc_summary[3, ] == max(mc_summary[3, ]))], "\n")

cat("Highest max     :", colnames(mc_summary)[which(mc_summary[4, ] == max(mc_summary[4, ]))], "\n")

cat("Lowest dao prob :", colnames(mc_summary)[which(mc_summary[5, ] == min(mc_summary[5, ]))], "\n")

cat("Lowest loss prob:", colnames(mc_summary)[which(mc_summary[6, ] == min(mc_summary[6, ]))], "\n")
```




# Appendix
## Average of returns vs returns of average

```{r}
x0 <- 100
y0 <- 200
Rx <- 0.5
Ry <- 1.5
```

Definition: `R = 1+r`

```{r}
cat(paste0("Let x_0 be ", x0, ".\n"))
cat(paste0("Let y_0 be ", y0, ".\n"))
cat("So the initial value of the pf is", x0 + y0, ".\n")
cat("\n")
cat(paste0("Let R_x be ", Rx, ".\n"))
cat(paste0("Let R_y be ", Ry, ".\n"))
```

Then, 
```{r}
cat(paste0("x_1 is R_x * x_0 = ", Rx * x0, ".\n"))
cat(paste0("y_1 is R_y * y_0 = ", Ry * y0, ".\n"))
```

Average of returns:  
```{r}
cat("0.5 * (R_x + R_y) =", 0.5 * (Rx + Ry), "\n")
```

So here the value of the pf at t=1 should be unchanged from t=0:  
```{r}
cat("(x_0 + y_0) * 0.5 * (R_x + R_y) =", (x0 + y0) * 0.5 * (Rx + Ry), "\n")
```

But this is clearly not the case:  
```{r}
cat("0.5 * (x_1 + y_1) = 0.5 * (R_x * x_0 + R_y * y_0) =", 0.5 * (Rx * x0 + Ry * y0), "\n")
```

Therefore we should take returns of average, not average of returns!  

Let's take the average of log returns instead:  
```{r}
cat("0.5 * (log(R_x) + log(R_y)) =", 0.5 * (log(Rx) + log(Ry)), "\n")
```

We now get:  
```{r}
cat("(x_0 + y_0) * exp(0.5 * (log(Rx) + log(Ry))) =", (x0 + y0) * exp(0.5 * (log(Rx) + log(Ry))), "\n")
```

So taking the average of log returns doesn't work either.



## Simulation of mix vs mix of simulations
Test if a simulation of a mix (average) of two returns series has the same distribution as a mix of two simulated returns series.

We are adding annual returns rather than multiplying, so imagine that we are simulating log returns.

```{r}
mc_sim <- function(
    num_runs = 1, 
    num_paths = 1000, 
    num_periods = 20,
    m_a = 0,
    s_a = 0.4,
    m_b = 10,
    s_b = 3
    ) {
  
  data_x <- rnorm(num_periods, m_a, s_a)
  data_y <- rnorm(num_periods, m_b, s_b)
  
  m_data_x <- mean(data_x)
  s_data_x <- sd(data_x)
  m_data_y <- mean(data_y)
  s_data_y <- sd(data_y)
  
  cat("m(data_x):", m_data_x, "\n")
  cat("s(data_x):", s_data_x, "\n")
  cat("m(data_y):", m_data_y, "\n")
  cat("s(data_y):", s_data_y, "\n")
  cat("\n")
  
  m_data_xy <- mean(0.5 * data_x + 0.5 * data_y)
  s_data_xy <- sd(0.5 * data_x + 0.5 * data_y)
  
  cat("m(data_x + data_y):", m_data_xy, "\n")
  cat("s(data_x + data_y):", s_data_xy, "\n")
  cat("\n")
  
  run_sim <- function(num_runs) {
    df <- data.frame(
      m_a = rep(0, num_runs), m_b = rep(0, num_runs), 
      s_a = rep(0, num_runs), s_b = rep(0, num_runs)
    )
    for(j in 1:num_runs) {
      sim_x <- rep(0, num_paths)
      sim_y <- rep(0, num_paths)
      sim_xy <- rep(0, num_paths)
      for(i in 1:num_paths) {
        sim_x[i] <- sum(rnorm(num_periods, m_data_x, s_data_x))
        sim_y[i] <- sum(rnorm(num_periods, m_data_y, s_data_y))
        sim_xy[i] <- sum(rnorm(num_periods, m_data_xy, s_data_xy))
      }

      df$m_a[j] <-  mean(0.5 * sim_x + 0.5 * sim_y)
      df$m_b[j] <-  mean(sim_xy)
      df$s_a[j] <-  sd(0.5 * sim_x + 0.5 * sim_y)
      df$s_b[j] <-  sd(sim_xy)
    }
    df
  }
  
  run_sim(num_runs)
}
```

```{r}
mc_sim_df <- mc_sim(
  num_runs = 10, 
  num_paths = 1000, 
  num_periods = 20,
  m_a = 0,
  s_a = 0.4,
  m_b = 10,
  s_b = 3
)
```

m and s of final state of all paths.  
`_a` is mix of simulated returns.  
`_b` is simulated mixed returns.  

```{r}
knitr::kable(mc_sim_df, digits = 3)
```

```{r}
summary(mc_sim_df)
```

`_a` and `_b` are very close to equal.  
We attribute the differences to differences in estimating the distributions in 
version a and b.  


The final state is independent of the order of the preceding steps:  

```{r}
vect1 <- c(rnorm(100))
vect2 <- c(sample(vect1, 100))
vect3 <- c(sample(vect1, 100))
path1 <- c(0, cumsum(vect1))
path2 <- c(0, cumsum(vect2))
path3 <- c(0, cumsum(vect3))
plot(path1, type = "l", col = "blue", 
     ylim = c(
       min(c(path1, path2, path3)), 
       max(c(path1, path2, path3))
     )
)
lines(path2, col = "red")
lines(path3, col = "green")
```


So does the order of the steps in the two processes matter, when mixing simulated returns?  

```{r}
vect1a <- c(rnorm(100, 0.05, 0.06))
vect1b <- c(sample(vect1a, 100))
vect2a <- c(rnorm(100, 0.05, 0.06))
vect2b <- c(sample(vect2a, 100))

path1a <- 100 * c(1, cumprod(1 + vect1a))
path1b <- 100 * c(1, cumprod(1 + vect1b))
path2a <- 100 * c(1, cumprod(1 + vect2a))
path2b <- 100 * c(1, cumprod(1 + vect2b))

mix_path_a <- 0.5 * path1a + 0.5 * path2a
mix_path_b <- 0.5 * path1b + 0.5 * path2b

plot(path1a, type = "l", lty = 1, col = "blue", 
     ylim = c(
       min(c(path1a, path1b, path2a, path2b)), 
       max(c(path1a, path1b, path2a, path2b))
     )
)
lines(path1b, lty = 2, col = "blue")
lines(path2a, lty = 1, col = "red")
lines(path2b, lty = 2, col = "red")
```

```{r}
plot(mix_path_a, type = "l", lty = 1, col = "blue", 
     ylim = c(
       min(c(mix_path_a, mix_path_b)), 
       max(c(mix_path_a, mix_path_b))
     )
)
lines(mix_path_b, lty = 2, col = "blue")
```

The order of steps in the individual paths do not matter, because the mix of simulated paths is a sum of a sum, so the order of terms doesn't affect the sum. If there is variation it is because the sets preceding steps are not the same. For instance, the steps between step 1 and 60 in the plot above are not the same for the two lines.

Recall,
$$\text{Var}(aX+bY) = a^2 \text{Var}(X) + b^2 \text{Var}(Y) + 2ab \text{Cov}(a, b)$$

```{r}
var(0.5 * vhr + 0.5 * phr)
0.5^2 * var(vhr) + 0.5^2 * var(phr) + 2 * 0.5 * 0.5 * cov(vhr, phr)
```


Our distribution estimate is based on 13 observations. Is that enough for a robust estimate?
What if we suddenly hit a year like 2008? How would that affect our estimate?  
Let's try to include the Velliv data from 2007-2010.  
We do this by sampling 13 observations from `vmrl`.  

```{r}
n <- 50
test_df <- data.frame(m = rep(0, n), s = rep(0, n))
for(i in 1:n) {
  vmrl_smp <- sample(vmrl, 13)
  test_df$m[i] <- mean(0.5 * vmrl_smp + 0.5 * phr)
  test_df$s[i] <- sd(0.5 * vmrl_smp + 0.5 * phr)
}
summary(test_df)
```





