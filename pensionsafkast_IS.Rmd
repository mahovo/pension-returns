---
title: "psnsionsafkast_IS"
output: html_document
date: "2024-04-14"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(fGarch)
```

```{r}
source("~/R\ work/pension-returns/pension-returns_functions.R")
#source("~/R\ work/pension-returns/importance-sampling.R")
```

$x_k$ is a stochastic variable from witch the $k$'th path is generated. $x_{i,k}$ is the $i$'th realization of $x_k$.  

$$\mathbf{x}_k \equiv \{x_{i,k}\}_{i=1}^n := \{\text{log-return}_{i,k}\}_{i=1}^n$$
The steps of the $k$'th path are produced by
$$\{S_{i, k}\}_{i=1}^n := \Big\{\sum_{j=1}^i x_i \Big\}_{i=1}^n  $$


$$h(\mathbf{x_k}) := \sum_{i=1}^i x_{i, k}$$
for the $k$'th path.


$f(x)$ is the densify of $x$, here a skewed student t-distribution.  

$g(z)$ is the proposed density, here a normal distribution.

$x{i,k}^g$ are values generated by $g$. 

$$E_f[h(x)] = \int h(x) f(x) dx \approx \dfrac{1}{m} \sum_{k=1}^m h(x_k) = \dfrac{1}{m} \sum_{k=1}^m \Big(\sum_{i=1}^n x_{i, k}\Big)$$
where $m$ is the number of paths, and $n$ is the number of steps in each path.


$z$ is a random variable with distribution $G(z) \equiv \int g(z)\ dz$. In other words, $\mathbf{z}_k \equiv \{z_{i,k}\}_{i=1}^n$ is generated by the quantile function $G^{-1}(p)$.

$w(z) := \dfrac{f(z)}{g(z)}$ are weights, one for each path.  

$$g^{*}(z) := \arg \min_g \text{Var}\big[ h(z) w(z) \big]$$

$$w_k^{*}(z) := \dfrac{f(z)}{g^{*}(z)}$$

$$E_g[h(z)] = \int h(z)\ w^{*}(z)\  g(z) dz \approx \dfrac{1}{m} \sum_{k=1}^m h(z_k)\ w^{*}(z) = \dfrac{1}{m} \sum_{k=1}^m \Big(\sum_{i=1}^n z_{i, k}\Big)$$



The density of $Y_1 + Y_2$ is the convolution of the densities $f_{Y_1}(\cdot)$ and $f_{Y_2}(\cdot)$.







```{r}
init_capital <- 100
num_prices <- 20
num_paths <- 1e2

num_steps <- num_prices - 1
```


```{r}
## Generate matrix of simulated X data
## Columns correspond to sample paths.
## num_steps is the length n of sample path.
## num_paths is the number of sample paths.
## rfunc is the random generating function, eg. runif, rnorm, etc.
## h(x) is simply log-returns, i.e. h(x) = x
x_mat_gen_sstd <- function(fit, num_steps, num_paths) {
  matrix(
    rsstd(num_steps * num_paths, fit$m, fit$s, fit$nu, fit$xi), 
    num_steps,
    num_paths, 
    byrow = FALSE
  )
}

# x_gen_func_sstd <- function(fit, num_steps = num_steps , init_capital = 100 ) {
#   init_capital * exp(cumsum(rsstd(num_prices, fit$m, fit$s, fit$nu, fit$xi))) ## Pf index values
# }

x_mat_gen <- function(num_steps, num_paths, rfunc = rsstd, ...) {
  matrix(rfunc(n = num_steps * num_paths, ...), num_steps, num_paths, byrow = FALSE)
}

# x_gen_func <- function(num_steps = num_steps, init_capital = 100, rfunc = rsstd, ...) {
#   init_capital * exp(cumsum(rfunc(...)))
# }

xg_mat_gen <- function(num_steps, num_paths, qfunc, ...) {
  num_p_vals <- num_steps * num_paths
  p_mat <- matrix(
    runif(num_steps * num_paths, 0.0, 1.0),
    num_steps, num_paths, byrow = FALSE
  )

  matrix(qfunc(p = p_mat, ...), dim(p_mat))
}

Sn <- function(x_vect) {
  sum(x_vect)
}

h_vect_gen <- function(x_mat, h_func = Sn) {
  ## For each column, apply h to the n x-values
  h_vect <- apply(x_mat, 2, h_func) ## 2 for columns
  h_vect
}
```



```{r}
x_mat <- x_mat_gen_sstd(
  fit = fit_mhr, 
  num_steps = num_steps, 
  num_paths = num_paths
)
```


Distribution of simulated log-returns
```{r}
hist(x_mat, breaks = 40)
```

```{r}
x_fit_normal <- MASS::fitdistr(x_mat, "normal")
x_fit_normal
```


```{r}
xg_mat <- xg_mat_gen(num_steps, num_paths, qfunc = qnorm, mean = log_ret_fit_normal$estimate[[1]], sd = log_ret_fit_normal$estimate[[2]])
```


```{r}
hist(xg_mat, breaks = 50, freq = FALSE, xlab = "log-returns", main = "Histogram of X_g")
```

```{r}
h_vect <- h_vect_gen(x_mat)
f_mat <- dsstd(x_mat, mean = fit_mhr$m, sd = fit_mhr$s, nu = fit_mhr$nu, xi = fit_mhr$xi)

#x_dens_data_norm <- dnorm(log_ret_seq, mean = x_fit_normal$estimate[[1]], sd = x_fit_normal$estimate[[2]])
hxf <- t(t(f_mat) * h_vect)

hxf_fit_normal <- MASS::fitdistr(hxf, "normal")
hxf_dens_data_norm <- dnorm(sort(hxf), mean = hxf_fit_normal$estimate[[1]], sd = hxf_fit_normal$estimate[[2]])

hxf_seq <- seq(min(hxf), max(hxf), length.out = num_steps * num_paths)

hist(hxf, breaks = 50, freq = FALSE, xlab = "log-returns", main = "Histogram of h(x)f(x) and density of g")
lines(x = hxf_seq, y = hxf_dens_data_norm, col="red")
rug(hxf)
```

```{r}
obj_func <- function(params, x_mat, h_vect, f_mat) {
  g_x_mat <- dnorm(x_mat, params[1], params[2])
  sd(h_vect * f_mat / g_x_mat)
}
```


Joint densities for each path:  

We need the self-convolution of a skewed student (or generalized) t distribution...  
See https://search.r-project.org/CRAN/refmans/bayesmeta/html/convolve.html 

We know (https://search.r-project.org/CRAN/refmans/bayesmeta/html/convolve.html) that the convolution of two normal distributions is a normal distribution with the mean and variance being the sums of the individual means and variances respectively. Let's do a quick and dirty test to see if this is true for a skewed Student t as well:

```{r}
params <- c(1, 2, 5, 1.5)
x_sstd_mat <- replicate(10000, rsstd(20, mean = params[1], sd = params[2], nu = params[3], xi = params[4]))
x_sstd_sums <- apply(x_sstd_mat, 2, sum)

loglik_sstd = function(beta, x) {sum(- dsstd(x, mean = beta[1], sd = beta[2], nu = beta[3], xi = beta[4], log = TRUE))}
start = c(mean(x_sstd_sums), sd(x_sstd_sums), 3, 1)
fit_sstd = optim(start, loglik_sstd, x = x_sstd_sums)
cat("params of terms:", params, "\n")
cat("params of sum:", fit_sstd$par, "\n")
```

19.89072 8.883992 33.12447 1.204468 


The means look about right.  
The variances:  
Is 8.88^2 = 20 * 2?
```{r}
8.883992^2
```
Close...

Unfortunately we only have to run the code a few times to see that $nu$ and $xi$ are not stable at all.

Can we compute the self-convolution of a skewed generalized t density function?  
A few pointers:  
+ the sum of two normally distributed random variables again turns out as normally distributed (with mean and variance resulting as the sums of the original ones).
      + https://search.r-project.org/CRAN/refmans/bayesmeta/html/convolve.html  
+ The distribution for independent returns over a period of time of n-days is an n-fold self-convolution of the distribution for a single day. A Student’s t-distribution (or q-Gaussian) is not self-replicating (stable) under self-convolution except in the case of an infinite number of degrees of freedom (which is a normal distribution) or for the number of degrees of freedom equal to unity (which is a Cauchy or Lorentzian distribution). Under self-convolution a Student’s t-distribution maintains the tails of the original distribution whereas the central part of the distribution changes
      + https://www.sciencedirect.com/science/article/abs/pii/S0378437111002329?via%3Dihub 
  
+ The pdf for n-fold self-convolution can be found by brute force calculation of the convolution integrals or as the (brute force) inverse Fourier transform of the nth power of the characteristic function of the original function.  
      + https://www.sciencedirect.com/science/article/abs/pii/S0378437111002329?via%3Dihub  
      + See also: S. Nadarajah, D.K. Dey, Convolutions of the T distribution, Computers and Mathematics with Applications  
+ If a random variable admits a probability density function, then the characteristic function is the Fourier transform (with sign reversal) of the probability density function.  
      + https://en.wikipedia.org/wiki/Characteristic_function_(probability_theory)  
      + See https://www.researchgate.net/publication/235386908_Characteristic_function_of_the_SGT_distribution  
            + ...but the very first sentence says "probability distribution function (PDF)"... I assume they mean probability density function...  
+ a key problem with moment-generating functions is that moments and the moment-generating function may not exist, as the integrals need not converge absolutely. By contrast, the characteristic function or Fourier transform always exists (because it is the integral of a bounded function on a space of finite measure), and for some purposes may be used instead.  
      + https://en.wikipedia.org/wiki/Moment-generating_function  
+ See package sgt:   
      + https://search.r-project.org/CRAN/refmans/sgt/html/sgt.html  





************* AT THIS POINT WE ARE A BIT STUCK........! *************




```{r}
f_n <- 
g_n <- 
```



```{r}
opt_params <- optim(c(0, 1), obj_func, x_mat = x_mat, h_vect = h_vect, f_mat = f_mat)
opt_m <- opt_params$par[1]
opt_sd <- opt_params$par[2]

g_star_mat <- dnorm(x_mat, opt_m, opt_sd)
w_star_mat <- f_mat / g_star_mat
```

```{r}
plot(h_vect * w_star_mat)
```






```{r}
  ## For each theta value:
  ## - Generate num_paths paths of num_steps steps
  ## - Calcuate a vector gn-density density values for that theta value
  sd_vect <- numeric(num_test_runs)
  g_dens_mat <- matrix(numeric(num_test_runs * num_paths), num_paths, num_test_runs)
  w_star_mat <- matrix(numeric(num_test_runs * num_paths), num_paths, num_test_runs)
  for (i in 1:num_test_runs) {
    xg_mat <- xg_mat_gen(num_steps, num_paths, qfunc = qnorm, mean = 0, sd = 1)
    g_dens <- apply(xg_mat, 2, function(x) {gn_1(x, theta = theta_vals[i], a = -1.9, b = 2)})
    f_dens <- replicate(num_paths, (1/3.9)^num_steps)
    #f_dens <- replicate(num_paths, (1/3.9))
    g_dens_mat[, i] <- g_dens
    w_star_mat[, i] <-  f_dens / g_dens
    sd_vect_tmp <- sd(h_vect_gen_3(xg_mat, default) * w_star_mat[, i])
    sd_vect[i] <- sd_vect_tmp
  }
```








# Notes

Some thoughts...  
Portfolio index value diffs are path dependent. For instance a diff of 100 is very unlikely, if the previous index value is 5. But if the previous value is 1000, a diff of 100 is quite likely.

So instead of making an MC simulation of diffs, we should make a simulation of log-returns. From that we can then calculate and plot paths of index values.



If the columns in `x_mat` are vectors of portfolio index value differences:

```{r}
diffs <- lapply(mc_mhr_a$mc_df, diff)
```


*** FIX ***
Here were are looking at the hist of $h$, but we should look at $h(x)\cdots f(x)$. We 

```{r}
hist(unlist(diffs), breaks = 40)
```

Even though the distribution is visibly skewed, we use a normal distribution as $g(x)$. This is because it is very easy to fit a symmetric normal distribution with `fitdistr()`. Also the thin tails should offer the benefit of not down weighting the tails as much.


```{r}
diffs_fit_normal <- MASS::fitdistr(unlist(diffs), "normal")
diffs_fit_normal
```


```{r}
diffs_seq <- min(unlist(diffs)):max(unlist(diffs))
diffs_dens_data_norm <- dnorm(diffs_seq, mean = diffs_fit_normal$estimate[[1]], sd = diffs_fit_normal$estimate[[2]])

hist(unlist(diffs), breaks = 50, freq = FALSE, xlab = "diffs", main = "Tæthed og histogram af diffs")
lines(x = diffs_seq, y = diffs_dens_data_norm, col="red")
rug(unlist(diffs))
```


Possible issue:  
The smallest value we will be deviding by is 1.212415e-29:
```{r}
min(diffs_dens_data_norm)
```

But we should try to minimize the sd of $h\cdot f/g^{*} \equiv h \cdot w^{*}$ wrt. the sd of the proposal distribution $g$

What happens if we use the distribution of $h \cdot f$ as $g$?  

$$\int \dfrac{h(x) f(x)}{h(x) f(x)}dx = \int 1 dx = x$$

